1.之前tf代码修改成pytorch有点问题，效果达不到tf得结果。

2.看了一篇论文。

Graph Convolution Machine for Context-aware Recommender System

Abstract

推荐的最新进展表明，通过在用户-物品交互图上执行图卷积可以学习到更好的用户和物品表示。然而，这样的发现主要局限于协作过滤(CF)场景，其中交互上下文不可用。在这项工作中，我们将图卷积的优势扩展到上下文感知推荐系统(CARS，它代表了一种可以处理各种侧面信息的通用类型的模型)。我们提出了图形卷积机(GCM)，这是一个端到端框架，由三个组件组成:编码器，图形卷积(GC)层和解码器。编码器将用户、项目和上下文投影到嵌入向量中，这些向量被传递给GC层，GC层在用户-项目图上使用上下文感知的图卷积来改进用户和项目嵌入。解码器通过考虑用户嵌入、项目嵌入和上下文嵌入之间的交互，对改进后的嵌入进行分解以输出预测分数。我们在来自Y help和Amazon的三个真实数据集上进行了实验，验证了GCM的有效性以及为CARS执行图卷积的好处。

3 Problem Definition

我们将CARS使用的数据分为四种类型:用户、项、上下文和交互。在[18]之后，我们将上下文定义为与交互相关的信息，例如，当前位置、时间、之前的单击等。图1展示了CARS中的数据，其中的主要数据是用户项-上下文交互张量。在稀疏张量中，每个非零条目(u, i, c)表示用户u在上下文c下与项目i进行了交互;我们给这样的项一个1的标签，即yuic = 1。每个u、i、c分别与multi-hot 特征向量u、i和c相关联，其中包含描述用户、项目和上下文的特征。例如，u包含静态用户配置文件(如性别和感兴趣的标签)，i包含静态项目属性(如类别和价格)，c包含动态上下文(如用户当前位置和时间)。

给定这样的数据，我们将其转换为具有相同表示能力的带有属性的用户-项目二部图的形式。具体来说，每个顶点代表一个用户或一个项目，每个边代表连接的用户和项目之间的交互。

每个顶点或边与特征向量u、i或c相关联。注意，在用户-项目对之间可能存在多条边，因为用户可能在不同的上下文中多次与相同的项目交互。我们将图中的所有边表示为集合Y = {(u, i, c)|yuic = 1}，用户u的邻居表示为集合Nu = {(i, c)|yuic = 1}，项目i的邻居表示为集合Ni = {(u, c)|yuic = 1}。

我们将cars问题表述为:

输入:用户-物品-上下文交互{(u, i, c)|yuic = 1}，用户{u}、物品{i}、上下文{c}的特征向量。

输出:预测函数f: u, i, c→R，它以用户、物品和上下文的特征向量作为输入，输出一个真实值，估计用户在上下文下与物品交互的可能性。

4 Graph Convolution Machine (GCM)

我们将在本节中介绍我们的方法。我们首先描述了预测模型，然后进行了模型复杂度分析和优化细节。

4.1 Predictive Model

图2演示了模型框架，它由三个组件组成:编码器、图形卷积层和解码器。接下来，我们逐一描述每个组件。

4.1.1 Encoder

编码器的输入有三个字段:用户字段特征u、项目字段特征i和上下文字段特征c。我们将ID特征包含在用户字段和项目字段特征中，因为当用户(项目)的配置文件(属性)相同时，它有助于区分用户(项目)。对于每个非零特征，我们将其与嵌入向量相关联，从而产生一组分别描述输入用户、项目和上下文的嵌入。然后，我们将用户(和项目)字段集合汇集到一个向量中，以便将该向量馈送到以下GC层，以改进用户(和项目)表示。具体来说，我们采用平均池化，即:

![image-20230420102141971](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230420102141971.png)

式中|u|为u中非零特征的个数，P∈RU×D为用户特征的嵌入矩阵，其中u为用户特征总数，D为嵌入大小。

P (0)u表示u的初始表示向量。类似地，我们得到项目i的初始表示向量为q(0) i。

请注意，这里可以应用其他池化机制，例如基于注意力的池化[17,39,40]，它可以为特征嵌入学习不同的权重。然而，我们尝试了一下，发现它并没有提高性能。因此，我们保持了最简单的平均池化，并避免引入额外的参数。因为我们没有在接下来的GC层中更新上下文表示，所以我们没有在context字段上执行池化。

我们将上下文域嵌入集表示为==Vc = {vs|s∈c}==，其中s∈c表示c中的非零特征，==vs表示上下文特征s的嵌入向量==。编码器输出p(0)u, q(0) i和Vc，它们被馈入GC层的下一个组件。

4.1.2 Graph Convolution Layers

这是GCM的核心组成部分，旨在解决现有基于监督学习的CARS模型的局限性。它通过利用整体用户-项目交互数据来改进p(0)u和q(0) i，这可以通过显式协同过滤信号[3]来增强用户和项目表示。用户项图上的GC通常被表述为一个消息传播框架:

![image-20230420102247961](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230420102247961.png)

式中p(l)u和q(l) i分别表示第l层GC的精细化用户表示和项目表示，g(·)是自定义函数。递归地进行这种消息传播，将用户的表示与其高阶邻居联系起来，例如，交互项的表示为一阶，协同交互用户的表示为二阶，这有利于协同过滤;同样的逻辑也适用于项目表示。

然而，标准GC不考虑边缘上的特征。在我们构建的用户-项目图中，用户和项目之间的边带有上下文特征，这对于理解依赖于上下文的交互模式非常重要。

例如，用户可能更喜欢在周五去酒吧，而在午餐时间去餐厅更受欢迎。因此，如果上下文特性可以适当地集成到GC中，就可以获得更好的用户和项表示。

为此，我们提出了一种新的GC操作，它结合了上下文的边缘特征:

![image-20230420102359609](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230420102359609.png)

接下来我们解释用户端GC的合理性，因为项目端可以用同样的方式解释。其中，|Nu|表示与用户u相连的边数，系数1√|Nu|是一个归一化项，以避免嵌入值的规模随着GC的增加而增加。我们通过平均上下文特征的嵌入并添加到连接的用户嵌入中来合并上下文特征。通过这种方式，我们建立了用户与其交互项目和交互上下文之间的连接。期望捕获这样的效果:如果用户喜欢在特定的上下文中选择一个项目，那么他们的表示之间的相似性是相似的。请注意，我们已经尝试了更复杂的机制，如合并Vc和q(1) i之间的成对相互作用，并使用MLP来捕获高阶相互作用。然而，这些方法并不能提高性能。因此，我们使用这种简单的平均运算，它易于解释和训练(不引入额外的参数)。

通过堆叠多个这样的GC层，用户(或项)表示可以通过其多跳邻居来改进。由于不同层的表示带有不同的语义，接下来我们将所有层的表示结合起来，形成一个更全面的表示:

![image-20230420102431626](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230420102431626.png)

其中αl为第l层表示的权值，可作为超参数，在αl≥0且PL l=0 αl = 1的约束下，通过网格搜索进行调优。然而，随着GCN的深入，调优它们的工作量呈指数级增长。在我们的实验中，我们发现将α 1设置为1/(L + 1)通常可以获得令人满意的性能。因此，为了简单起见，我们将αl固定为1/(L + 1)。一个可能的扩展是学习α 1，例如，在验证数据上设计注意力机制或优化它们。我们把这个扩展作为未来的工作，因为它不是这项工作的重点。

在下文中，我们将提供用于实现的GC层的矩阵形式。设用户-物品交互矩阵为Rui∈RN×M，其中N和M分别表示用户数量和物品数量。每个条目rui∈Rui是用户u与项目i交互的次数。同样，我们用Ruc∈RN×K和Ric∈RM×K分别表示用户-情境交互矩阵和项目-情境交互矩阵，其中K为情境的个数。然后定义用户-项目-上下文图的邻接矩阵为

![image-20230420102507306](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230420102507306.png)

其中0是全零矩阵，I是单位矩阵。设D为A的对角度矩阵，即第t个对角元素Dtt = P j At j。归一化邻接矩阵可表示为

![image-20230420102522257](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230420102522257.png)

则得到逐层传播规则的矩阵形式，等价于式(3):

![image-20230420102535770](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230420102535770.png)

其中E(l)∈R(N+M+K)×D是用户、项目和上下文的连接嵌入矩阵。设E(0)为Encoder中编码嵌入表的连接矩阵，可以表示为

![image-20230420102550515](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230420102550515.png)

最后，我们得到最终的嵌入矩阵

![image-20230420102602681](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230420102602681.png)

4.1.3 Decoder

GC层输出用户pu和项目qi的精细化表示，并保持上下文特征的嵌入不变。解码器的作用是通过接受表示来输出预测分数。解码器的标准选择是多层感知器(MLP)，但由于它仅以隐式方式建模特征交互，因此在这里存在不足。在CARS中，对特征之间的交互进行显式建模对于用户偏好估计非常重要。例如，经典的因子分解机(FM)对特征嵌入之间的成对交互进行建模，长期以来一直是CARS的竞争模型。

受FM的简单性(线性模型)和有效性的启发，我们采用FM作为GCM的解码器。其思想是显式地对用户、项目和上下文的(精细化的)表示之间的成对交互进行建模，并使用内积。具体来说，设向量集合V为Vc∪{pu, qi}，则解码器将预测分数输出为:

![image-20230420102650114](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230420102650114.png)

这里排除了自相互作用的vTs vs，因为它们对预测毫无用处。为了清晰起见，省略了每个用户、项目和上下文特征的偏差项。

请注意，我们基于FM的解码器与普通FM略有不同，后者对所有输入特征嵌入之间的相互作用进行建模。在这里，我们将每个用户(项目)投影到一个向量中，而不是保留其特征的嵌入。这种方式的一个优点是放弃了用户字段(item-field)特性的内部交互，更多地揭示了用户(item)和上下文特性之间的交互，正如预期的那样。

4.3 Optimization

为了优化模型参数，我们选择了point-wise log loss，这是推荐系统中常见的选择[1,16]。在每个训练历元中，我们随机抽取Y中每个实例的未观察到的交互，形成负集Y−，即对于Yelp(或Amazon)数据集的每个观察到的实例(u, i, c)∈Y，我们随机匹配用户u在上下文c下没有交互的项目池中的4(或2)个项目。然后最小化以下目标函数:

![image-20230420102937507](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230420102937507.png)

其中σ(·)为sigmoid函数，λ控制L2正则化以防止过拟合。优化是由小型批处理Adam[41]完成的。

 
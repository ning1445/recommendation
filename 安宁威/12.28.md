目前还在看代码

Dataset for compatibility matching.我们将推荐数据集中的正面outfit作为兼容性匹配的训练集，另外选取6,924套(占训练数据集的10%)从未出现在训练集中的outfit构建测试数据集。统计数据如表2所示。为了评估HFGN在兼容性匹配上的性能，我们采用了一个广泛使用的任务，填空[12]。

Fill-in-the-Blank (FLTB).fill -in-the- blank (FLTB)[12]是从候选项目中选择最兼容的项目来填充一套outfit的空白，如图4所示。对于测试数据集中的每一套衣服，我们随机将一件 item掩码为空白，然后从其他outfit中随机选择3件item，与被掩码的item组成候选集。我们将masked item设置为ground truth，假设masked item比其他候选项更兼容。

对于测试数据集中的每一件outfit，我们随机选择一个item作为空白，并设置三个负候选。目标是从四名考生中选择正确的答案，填写outfit中masked的空白。我们报告准确性来评估性能。



###                          基于异质信息网络表示学习的论文推荐研究

​                                                                                摘要

面对互联网中海量的学术信息，研究者们如何能快速精准地找到符合自己研究兴趣和研究主题的学术文章是一个亟待解决的问题，推荐技术是解决这一问题的典型策略。学术网络作为一种异质信息网络，其中存在多种类型的节点和连接关系，如何在推荐中利用研究者和论文信息的同时，保留网络中信息的完整性并充分学习节点之间的关系是论文推荐面临的挑战性问题。然而，当前主流的推荐技术大多只考虑文本信息或引文信息，对学术网络中交错的关系信息考虑不充分。针对以上问题，本文引入异质信息网络表示学习开展研究，综合考虑**学术网络中论文、作者、主题以及会议等类型节点的信息**和**不同关系类型的语义信息**。

​		本文提出两种新颖的表示学习策略用于解决学术网络中信息的表示学习，主要有以下贡献：(1) 针对学术网络中节点类型多元化和节点之间路径影响力不同的特点，本文提出一种融合节点权重和路径权重的异质信息网络表示学习方法，根据节点邻域和不同类型的元路径，计算双重权重信息，突出不同节点在元路径下的重要性以及各类型元路径的不同重要程度，提高了论文推荐性能。(2) 针对异质信息网络中信息无法充分捕捉对论文推荐性能的影响，本文设计了一种嵌入子网络的异质信息网络学习方法，该方法构建了节点特征子网络和路径子网络，通过设计的节点关系增强注意力融合两种子网络中的信息，避免信息丢失，建模学术网络中的关系，提升了论文推荐服务质量。(3) 在两个公开真实数据集上，对本文提出的两种表示学习策略进行实验，通过大量的对比实验，将本文提出的模型与基线模型进行对比分析，证明本文所提异质信息网络表示学习方法能够更好地融入论文推荐中，从而提升了推荐的性能。

第三章 融合节点权重以及元路径权重的论文推荐

​		本文设计了一个融合节点和元路径权重的双重注意力模型(Fusing Node and Metapath Weights Dual Attention Model, FWDA)，用于解决异质信息网络中节点和路径的权重问题。该模型不仅考虑了节点和路径的特征信息，更是融合了节点和路径的权重信息，保留学术网络中信息的同时，添加权重衡量不同关系类型的信息。模型主要包含节点重要性建模层、元路径重要性建模层和预测层。FWDA的整体框架如图 3-1 所示：

![image-20221228111234927](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228111234927.png)

3.2.1 节点重要性建模

​		节点重要性建模层是为节点以及邻居节点分配权重，节点及其邻域节点之间的邻近度有所不同，如图 3-2 所示，作者 A 1 和作者 A 4 没有共同合作的论文，在一阶邻近度上两者邻近度为 0。然而，A 1 与 A 2 、A 3 有共同合作关系，即 A 2 、A 3 是作者 A 1 的邻居节点，同时，A 2 、A 3 又是作者 A 4 的邻居节点，由此可见，作者 A 1 和 A 4 虽没有直接相连，但是两者拥有共同邻域，他们之间的二阶邻近度不为 0。即表明，两节点之间即使没有直接相连，但也有高阶关系，只是互相之间的影响力程度与直连节点有所不同。

![image-20221228111758117](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228111758117.png)

​		在学术网络中，任一个节点都存在各自的邻域，节点邻域中的节点影响力不同。例如，在某一条元路径 APA 下，对于论文 P 节点，有撰写此论文的研究者 A 1 ，有与A 1 合作和其他研究者 A i ，显然，对于论文 P 来说，作者列表中的研究者对其贡献程度是不同的，即表明对于节点 P 来说，其邻居节点的影响力是不同的，因此，需要为邻域集合中的邻居节点分配不同的权重，以此来权衡节点之间的影响力。

![image-20221228111945636](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228111945636.png)

​		给定一个特定类型的元路径，本文采用一个基础注意力机制学习该路径下的节点权重，注意力机制结构如图 3-3 所示。首先，对每一个样本，有一个dq 维的查询向量，构成m×dq维的查询向量矩阵 Q，即**查询向量认为是样本的特征**。此外，对于每一条信息，都有一个dk 维的键向量和dv 维的值向量，构成一个键-值对。若有 m 条信息，即是有 m 个向量，则构成m×dk 维键向量矩阵 K 和m×dv 维的值向量矩阵 V。注意力机制的最终结果：

![image-20221228112304055](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228112304055.png)

​		其中，![image-20221228112344439](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228112344439.png)为比例因子，除以![image-20221228112409368](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228112409368.png)能够保证模型训练过程中梯度值保持稳定。特定路径下节点权重信息计算如下：

![image-20221228112446631](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228112446631.png)

​		其中，xi和xj 表示节点 i 和 j 的低维节点特征，![image-20221228112537663](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228112537663.png)表示某种特定的元路径类型，![image-20221228112556836](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228112556836.png)表示节点 i 的邻域，![image-20221228112629432](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228112629432.png) 表示 ![image-20221228112537663](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228112537663.png) 类型路径下的注意力系数。最后，对节点进行带权融合得到：

![image-20221228112714968](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228112714968.png)

​		![image-20221228112747942](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228112747942.png) 即是节点 i 在路径![image-20221228112537663](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228112537663.png)下的表示，是其邻居节点的带权融合。由此可以得到任意节点在特定路径下的表示，以此类推可以获得不同类型元路径下的节点表示，之后输入到路径重要性建模层，融合多条元路径学习最终嵌入。

3.2.2 元路径重要性建模

​		不同的元路径包含有不同的语义信息，这些信息也具有不同的重要性，例如，图3-2 所示的节点邻域示意图中，研究者 A 1 在 APA 路径类型下，经过 A 1 →P 2 →A 3 →P 3→A 1 寻找到的论文 P 3       与 A 1 在 APTPA 路径下，经过 A 1 →P 1 →T 1 →P 4 →A 1 寻找到的论文 P 4 相比，P 4 与研究者 A 1 的论文相关性更强，即路径类型 APTPA 的说服力更强一些，包含的语义信息更加重要，则在权重计算时，APTPA 类型的路径自然需要赋予更高的权值。在节点重要性建模中，给定元路径类型，计算元路径上节点之间的不同影响力权值，并对这些节点进行加权融合。之后，将融合邻域权值信息的节点表示作为语义信息权重建模层的输入，学习不同语义的元路径权重信息。

​		为了建模元路径的重要性，引入了自注意力机制学习语义的权值信息并进行融合。自注意力机制是在基础注意力机制上进行改良，聚焦于捕捉数据和特征的内部关系，而基础注意力机制对外部信息有较强的依赖。在自注意力机制中，将输入特征自身作为注意力机制的输入：

![image-20221228113335623](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228113335623.png)

​		其中，WQ ，WK ，WV 是映射矩阵。将自注意力网络表示为如下形式：

![image-20221228113415180](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228113415180.png)

​		在本文模型的路径重要性建模过程中，将节点重要性建模层输出的节点嵌入表示输入到元路径级的自注意力网络中进行语义权值信息的学习。若在学术网络中，有 m种路径类型，则经过上述的节点权重建模之后，可以得到 m 组在特定语义下的带权节点表示![image-20221228114034207](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228114034207.png) 。将节点表示输入到元路径重要性重建模层，利用注意力机制分配权重，从而能够自适应地关注元路径包含的重要语义信息。经过元路径重要性建模生成最终的嵌入表示 ：

![image-20221228113429314](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228113429314.png)

​		将得到的嵌入表示信息输入到预测层进行预测任务：

![image-20221228113438154](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228113438154.png)

​		其中，C 为预测层参数。最小化目标函数![image-20221228113501154](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228113501154.png)来优化整个模型。

第四章 嵌入子网络的异质信息网络学习模型

​		上一章中，介绍了融合节点和元路径权重信息的论文推荐策略，同时与一些具有代表性的基线方法进行了对比，本文的模型在推荐性能上有所增强，但是该模型和前文的一些相关技术都是**只考虑了序列性的节点嵌入和元路径集合，将各个节点和路径做单独处理，虽然能够在网络结构性上保留一定的邻域影响力信息，但是依然是依赖于初始的网络进行信息处理的方式，**对于学术网络的网络==结构学习并不充分==，本章着重于异质信息网络结构学习，进一步挖掘学术信息网络的节点和关系之间的结构性信息，设计一个**嵌入子网络的异质信息网络结构学习模型**(HIN Structure Learning ModelEmbedding Sub-network, HIN-SLES)，对异质信息网络进行表示学习。在对异质信息网络进行网络嵌入时，构建节点的子网络和路径子网络，而后融合两种子网络进行下游任务。通过实验验证，本章所提出的方法能够进一步提高模型的准确性，对于论文推荐性能的提升有较好的推动作用。

节点特征及路径子网络嵌入

​		本小节将详细介绍本章的 HIN-SLES 模型框架，主要包括节点特征及路径子网络的构建层、节点关系增强注意力层以及预测层几个部分。首先建模节点特征子网络，提取节点之间的结构性信息，之后与路径子网络进行联合学习，充分捕捉网络整体性信息。下面将对这几部分进行详细介绍，模型的整体框架如图 4-1 所示。

![image-20221228175516257](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228175516257.png)

4.2.1 子网络构建
(1) 节点特征子网络

![image-20221228175759958](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228175759958.png)

​		在此部分，利用节点特征之间的相似度构建节点子网络，通过挖掘节点特征，学习节点之间的潜在关系。在异质信息网络中，由于节点与节点之间存在不同类型的关系而形成错综复杂的连接，如图 4-2 所示，在这个可视化的示意图中，对于论文 P 来说，节点 P 1 、P 2 、P 3 与节点 P 之间有不同的连接关系，相应的，它们与论文 P 存在不同的相似性。以此类推，对于不同类型的节点，它们之间同样有不同的相似程度，即在异质信息网络中所有节点在一阶或高阶关系上，都因存在不同的相似度而互相有不同的影响力。那么考虑节点之间的相似度 S，以此来构建节点特征相似度网络，进行节点的嵌入。在这个子网络中，将节点与节点之间的连接映射成节点之间的相似程度，这个过程即挖掘节点间的结构性信息，构建成节点特征子网络从而进行下一步的嵌入。

​		首先，对学术网络中的节点，先将其映射到低维向量空间中进行节点特征表示，之后通过相似度计算公式计算节点之间的特征相似程度。在向量空间中，通常认为两个特征向量越相近，两者越相似，空间中相距较远的向量则认定为不相似，如图 4-3所示。

![image-20221228175741442](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228175741442.png)

​		在推荐领域，使用最多的是余弦相似度。在本文中，是需要计算两个节点之间的相似度用以构建特征子网络，故采用余弦相似度计算节点之间的特征相似性：

![image-20221228180047666](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228180047666.png)

其中，yi和yi 代表节点的特征向量，wn 表示相似度函数的可学习系数矩阵。计算节点之间的相似度之后，根据相似度度量，构建节点的特征相似性子网络：

![image-20221228180105930](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228180105930.png)

其中，kn 表示生成节点特征子网络的阈值，小于阈值的节点判定为不相似节点，则排除在节点特征相似子网络之外。

(2) 路径子网络

​		路径子网络是根据学术网络中的高阶拓扑结构构建，节点之间不同的连接关系形成不同的路径。采用 Metapath2vec 的方法生成路径表示矩阵，思想与 Deepwalk 类似，不同的是，Metapath2vec 中考虑了异质信息网络中节点的不同类型。对于给定的异质信息网络)G= (V E T )，为了学习基于元路径的 d 维潜在表示矩阵，首先在异质信息网络中进行特定元路径下的随机游走，获取节点序列 ![image-20221228191237554](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191237554.png)，例如，在学术网络中，存在四种类型的节点，设定游走类别为 APA、APTPA 和 APCPA 几种类型。获取到特定元路径类型下的节点序列之后，在经过 Skip-gram 优化序列，生成一系列嵌入矩阵![image-20221228191319852](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191319852.png) ，其中![image-20221228191340844](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191340844.png)即基于元路径的语义表示矩阵 。

​		生成表示矩阵之后，与节点特征子网络类似，经过度量学习设定阈值pk ，生成每条元路径下的语义邻接矩阵：

![image-20221228191410722](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191410722.png)

![image-20221228191431257](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191431257.png)同样为带有可学习矩阵w p的度量函数。经过此过程，可以得到一系列语义邻接矩阵 ![image-20221228191506686](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191506686.png)。不同的元路径同样也有不同的重要性，通过路径聚合函数![image-20221228191523175](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191523175.png)可为每条元路径赋予不同的关注度，那么通过这样的方式聚合每条元路径下的邻接矩阵构建路径子网络：

![image-20221228191536758](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191536758.png)

4.2.2 节点关系增强注意力

​		首先通过子网络融合函数![image-20221228191010886](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191010886.png)将子网络构建模块生成的节点特征子网络和路径子网络进行融合：

![image-20221228191023554](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191023554.png)

​		之后，应用图注意力学习 g 中节点的权重，对不同节点与相邻节点的关系赋予不同的关注度，重点关注有较大作用的节点和路径关系，从而强化重要性节点和关系信息对性能的影响。此种方法不仅仅是对节点进行加权，而且能够关注到网络的整体信息。对于融合节点特征子网络及路径子网络后的网络 g ，强化 g 中节点之间的关系，计算权值系数：

![image-20221228191635530](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191635530.png)

其中，A 是一个权值矩阵，W 为变换矩阵，并引入非线性激活函数 LeakReLu。之后执行加权融合操作，获得网络的嵌入：

![image-20221228191650750](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191650750.png)

经过设计的节点关系增强注意力，将节点特征子网络和路径子网络融合后，对节点间的关系进行强化，关注到网络的整体性信息。这种方法很大程度上保持了网络中信息的结构性，使得模型的预测能力能够有较大提升。

经过上述网络嵌入过程，通过设计的节点关系增强注意力网络将构建的两个子网络进行融合并通过计算权重系数为不同的节点关系分配不同的权重，突出重要的节点关系。由此一来，能够在进行网络嵌入过程中，处理局部信息的同时也能够关注到整体的信息。

将获得的嵌入信息输入到预测层产生预测：

![image-20221228191730848](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191730848.png)

其中，C 为预测层参数。最后，采用交叉熵作为损失函数：

![image-20221228191743126](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221228191743126.png)
代码整体可以跑通，测试部分还有一些问题没有解决

![image-20221231113947582](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20221231113947582.png)

![image-20230101193625391](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230101193625391.png)

```
if __name__ == '__main__':
    # args = parse_args()
    #指定GPU设备号
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_id)  # args defined in bath_test.py

    config = dict()

    #'../Data/'   +   'pog'
    data_generator = Dataset.Data(path=args.data_path + args.dataset)
    config['n_users'] = data_generator.n_users#140
    config['n_outfits'] = data_generator.n_train_outfits#101
    config['n_items'] = data_generator.n_all_items#51
    config['n_cates'] = data_generator.n_cates#9
    config['max_ol'] = data_generator.max_ol#9
    config['visual_feat'] = data_generator.visual_feat#【51，2048】
    config['vf_dim'] = data_generator.vf_dim#2048
    config['cate_items'] = data_generator.cate_item_dict#字典{cate：[item,item...]
    config['norm_uo_adj'] = data_generator.norm_uo_adj#归一化uo邻接矩阵

    gather_index = _get_gather_index(data_generator.cate_item_dict)
```

将x中的元素从小到大排列，提取其对应的index(索引)，然后输出

```
def _get_gather_index(cate_items):
    index_list = []
    #len(cate_items)=9
    for c in range(len(cate_items)):
        items = cate_items[c]
        index_list.append(items)

    index_ = np.concatenate(index_list, axis=0)
    #argsort()函数是将x中的元素从小到大排列，提取其对应的index(索引)，然后输出到y
    gather_index = np.argsort(index_)

    return gather_index
```

![image-20230101195333699](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230101195333699.png)

选择优化loss及是否加载预训练数据

```
t0 = time()

#0: optimize one loss; 1: optimize two loss
if args.train_mode == 0:
    print('emb dim', args.embed_size, 'lr', args.lr)
elif args.train_mode == 1:
    print('emb dim', args.embed_size, 'fltb lr', args.fltb_lr, 'recommend lr', args.recom_lr)

#0: No pretrain, -1: Pretrain with the learned embeddings, 1:Pretrain with stored models.
if args.pretrain == -1:
    pretrain_data = load_pretrained_data()
else:
    pretrain_data = None

model = HFGN(data_config=config, pretrain_data=pretrain_data)
```

是否保存model参数

```
#0: Disable model saver, 1: Activate model saver
if args.save_flag == 1: # layer_size: output sizes of every layer.
    if args.train_mode == 0:
        weights_save_path = '%sweights/%s/l%s' % (
            args.weights_path, model.model_type, str(args.lr))
    elif args.train_mode == 1:
        weights_save_path = '%sweights/%s/fl%s_rl%s' % (
            args.weights_path, model.model_type, str(args.fltb_lr),
        str(args.recom_lr))
    ensureDir(weights_save_path)

    save_saver = tf.train.Saver(model.save_weights, max_to_keep=1)

config = tf.ConfigProto()#对session进行参数配置
config.gpu_options.allow_growth = True##动态申请显存
sess = tf.Session(config=config)##让参数设置生效
```

重新加载预训练的模型参数。

```
if args.pretrain == 1:

    pretrain_path = args.pretrain_path

    ckpt = tf.train.get_checkpoint_state(os.path.dirname(pretrain_path + '/checkpoint'))
    if ckpt and ckpt.model_checkpoint_path:
        sess.run(tf.global_variables_initializer())
        save_saver.restore(sess, ckpt.model_checkpoint_path)
        print('load the pretrained model parameters from: ', pretrain_path)

    else:
        sess.run(tf.global_variables_initializer())
        cur_best_hit_0 = 0.
        print('without pretraining.')

else:
    sess.run(tf.global_variables_initializer())
    cur_best_hit_0 = 0.
    print('without pretraining.')

# 51X2048 item特征
sess.run(model.assign_feat, feed_dict={model._init_visual_feat: data_generator.visual_feat})
#101X9                                                 一套ouifit对应的item
sess.run(model.assign_map, feed_dict={model._init_outfit_map: data_generator.outfit_map})
#101                                                     每个ouifit长度
sess.run(model.assign_length,feed_dict={model._init_outfit_len: data_generator.outfit_len})
#51                                           将x中的元素从小到大排列，提取其对应的index(索引)
sess.run(model.assign_gather, feed_dict={model._init_gather_index:gather_index})
#101X9X9
sess.run(model.assign_adj, feed_dict={model._init_outfit_adj:data_generator.outfit_adj})
for c in range(data_generator.n_cates):
    sess.run(model.assign_cate_index[c], feed_dict={model._init_cate_index[c]: data_generator.cate_item_dict[c]})
```

训练及输出loss

```
#评价指标
pre_loger, rec_loger, ndcg_loger, hit_loger = [], [], [], []
fltb_auc_loger = []
stopping_step = 0
should_stop = False

for epoch in range(args.epoch):  # args.epoch
    t1 = time()

    recom_loss, mf_loss, reg_loss, fltb_loss = 0., 0., 0., 0.
    if args.train_mode == 0:
        '''Train.'''
        t1 = time()
        # generate all batches for the current epoch.
        batch_begin = time()
        batches = batch_generator.sample(data_generator=data_generator, batch_size=args.batch_size)
        batch_time = time() - batch_begin
        if epoch == 0:
            print("batch_time", batch_time)

        num_batch = len(batches[1])
        batch_index = list(range(num_batch))
        np.random.shuffle(batch_index)


        for idx in tqdm(batch_index, ascii=True):
            u_batch, po_batch, plen_batch, no_batch, nlen_batch, \
            f_batch, flen_batch, fadj_batch = batch_generator.batch_get(batches, idx)

            _, batch_loss, batch_recom_loss, batch_mf_loss, batch_reg_loss, batch_fltb_loss = sess.run(
                [model.opt, model.loss, model.recom_loss, model.mf_loss, model.reg_loss, model.fltb_loss],
                feed_dict={model.user_input: u_batch, model.po_input: po_batch,
                           model.pl_input: plen_batch, model.no_input: no_batch, model.nl_input: nlen_batch,
                           model.fltb_input: f_batch, model.flen_input: flen_batch, model.fadj_input: fadj_batch,
                           model.node_dropout: eval(args.node_dropout),
                           model.mess_dropout: eval(args.mess_dropout)
                           })

            recom_loss += batch_loss
            mf_loss += batch_mf_loss
            reg_loss += batch_reg_loss
            fltb_loss += batch_fltb_loss

#输出loss
        if args.verbose > 0 and epoch % args.verbose == 0:
            perf_str = 'Epoch %d [%.1fs] train==[loss=%.5f mf_loss= %.5f, reg_loss=%.5f, fltb_loss=%.5f]' % (
            epoch, time() - t1, recom_loss, mf_loss, reg_loss, fltb_loss)
            print(perf_str)
```






















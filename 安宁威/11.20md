**Structured Graph Convolutional Networks with Stochastic Masks for Recommender Systems**

http://yusanlin.com/files/papers/sigir21_structure.pdf

ABSTRACT

图卷积网络（GCN）在协同过滤方面功能强大。GCN的关键组成部分是探索邻域聚合机制，以提取用户和项目的高级表示。然而，现实世界中的用户项目图通常是不完整的，并且很嘈杂。如果GCN没有得到适当的正则化，聚集误导性的邻域信息可能会导致次优性能。此外，真实世界中的用户项目图通常是稀疏的和**低秩**的。这两个内在的图特性在图神经模型中研究较少。 

在这里，我们提出了结构化图卷积网络（SGCN），**通过利用稀疏性和低秩的图结构特性来提高GCN的性能**。为了实现稀疏性，我们将GCN的每一层附加一个可训练的随机二进制掩码，以修剪有噪声和不重要的边，从而得到一个干净和稀疏的图。为了保持其低秩性质，应用核范数正则化。我们通过求解随机二元优化问题，联合学习随机二进制掩码和原始GCN的参数。进一步提出了一种无偏梯度估计器，以更好地反向传播二进制变量的梯度。

> 在协同过滤中，低秩体现在相似行为的用户对于商品有着相似的评分。

1 INTRODUCTION

个性化推荐系统已广泛部署在许多在线服务中，以满足用户的兴趣[47]。最突出的技术之一是协同过滤，它考虑用户的历史交互，并假设过去有相似偏好的用户在未来会做出类似的决定。因子分解机器通过使用用户嵌入和项目嵌入的内积作为偏好得分而取得了巨大成功[23]。然而，由于缺乏学习高阶用户项目特征交互的策略，他们的推荐性能不令人满意[5，15，18，26，27，51]。因此，深度学习技术已经开始主导推荐系统的格局[48]。 

最近，图卷积网络（GCN）在图结构数据的表示学习中变得越来越强大[16，22，44]。GCN在输入图上使用消息传递机制，可以概括为三个步骤：1）用节点的初始属性或结构特征（如节点度）初始化节点表示；2） 通过递归聚合和变换其邻居的表示来更新每个节点的表示；3） 根据下游任务的要求，读取单个节点或整个图形的最终表示。通过将用户-项目交互视为一个二分图，研究人员试图采用GCN进行推荐，因为其理论优雅且性能良好[11，17，30，42，47]。例如，PinSage[47]将有效的随机游走和图卷积结合起来生成项目嵌入。NGCF[42]提出了一种嵌入传播层来研究二部图中的高阶连通性。LightGCN[17]简化了GCN的设计，使其更简洁，便于推荐。 

> 在无向图中，某个节点的度等于与该节点直接相连节点的数量；在有向图中还需要考虑入度和出度。

尽管已经取得了令人鼓舞的性能，但由于其递归消息传递模式，GCN仍然容易受到输入图质量的影响[7，52]。不幸的是，现实世界中的用户项目图通常很嘈杂。这对于隐式行为尤其如此，因为它们不一定与用户偏好一致[37]。如果GCN没有得到适当的正则化，聚集误导性的邻域信息可能会导致次优性能。我们使用下面的例子来进一步解释上面提到的问题。 

图1说明了误导性信息是如何通过图中的噪声边传播的。这里我们考虑在用户-物品图中嵌入目标节点u1，用户u1和物品i4之间有一个噪声边，如图左子图所示。右边的子图是相应的树结构，节点u1是根。GCNs的核心思想是充分发现二部图中的高阶关系。因此，即使u1和i2之间没有显式连接，也可以通过路径u1←i3←u3←i2对节点i2的表示进行聚合，更新目标节点u1的表示。然而，误导性的信息，例如嵌入第一跳邻居i4或第二跳邻居u2，也可以通过有噪声的边缘u1−i4传递到目标节点u1，这可能会降低性能。随着GCN的深入，这些误导性的信息将继续传播并污染整个图。

![image-20221121150430648](https://gitee.com/ning13445/picture/raw/master/1/image-20221121150430648.png)

为此，我们认为在消息传递过程中删除不相关的邻居是至关重要的。否则，包括不太有用的信息将使模型训练复杂化，增加过度拟合的风险，甚至损害模型的有效性。关键的挑战是确定在训练阶段忽略不相关邻居的标准。幸运的是，真实世界的图通常是稀疏的和低秩的[12]。**稀疏意味着在消息传递期间，只有最重要的邻居才应该本地连接到目标节点；低秩约束表示整个图是全局结构化的，只有少数因素会影响用户的偏好。这两个内在图特性广泛用于线性矩阵完成模型**[4，23，32]，例如。lp 范数正则化或矩阵秩最小化，但在图神经模型中研究得很少。一种可能的方法是首先根据某种相似性函数创建一个干净的 k-近邻图。这是在LLE[36]和Isomap[39]等阴影图模型中常用的策略，最近在深度图模型[49]中也被重新审视。然而，k-近邻的表达能力受限于k的选择以及嵌入空间中的相似性函数。

Present Work. 

在这里，我们提出了一种结构化图卷积网络（SGCN），通过利用稀疏性和低秩的图结构特性来提高GCN的性能。为了实现稀疏性，我们在GCN的框架下，用随机二元掩码连接GCN的每一层，以修剪噪声和不重要的边缘。直观地说，随机二进制掩码（即，1被采样，0被丢弃）可以被视为图生成器，以支持GCN的每一层的高质量稀疏图。我们的动机有两方面：1）可以学习以数据驱动的方式丢弃带有参数化mask的噪声边。因此，稀疏消息传递策略不那么复杂，并且具有更好的泛化能力；2） 过拟合和过平滑是开发更深层次GCN的两个主要瓶颈[35]。这些问题可以通过使用我们的随机机制对子图进行采样来缓解。然而，由于离散样本的组合性质，直接训练随机二进制掩码是困难的。为了使它们可微，我们进一步**通过概率重参数化将优化问题从离散空间重新表述为连续空间**[20，46]。进一步提出了一种无偏梯度估计器，以更好地反向传播二进制变量的梯度。受对抗性机器学习[9，21]的启发，**低秩约束也被施加到GCN的每一层的稀疏邻接矩阵上。这种正则化迫使图被全局结构化，**这已被证明在防御对抗性攻击方面非常成功[9，21]，更不用说防御推荐中的噪声了。我们进行了大量实验，以评估所提出的SGCN方法的有效性和鲁棒性。我们的贡献如下： 

•我们提出了SGCN，这是一种在GCN的消息传递阶段明确地修剪不相关邻居的方法，这在很大程度上减少了推荐系统中噪声的负面影响。 

•我们开发随机二进制掩码，目的是为GCN的每一层选择稀疏且高质量的子图。还施加了**低秩约束以增强GCN的鲁棒性和泛化**。 

•我们提出了一种用于随机二元优化的无偏梯度估计器，方法是将其转化为连续空间中的等效梯度。因此，我们可以共同学习随机二进制掩码和GCN的参数。 

•我们对四个公共数据集进行了广泛的实验。结果表明，SGCN在修剪噪声边缘和使用低秩约束的有效性方面具有优势，结果为4.92%∼ 26.23%的业绩增长。 

2 RELATED WORK

在本节中，我们简要回顾了推荐系统和图卷积网络的相关工作。我们还强调了现有努力与SGCN之间的差异。 

2.1 Collaborative Filtering

推荐系统通常使用协同过滤（CF）来基于用户的历史配置文件学习用户和项目之间复杂的特征交互。矩阵分解是一种早期方法，用于从用户项目评级矩阵中学习用户和项目的潜在嵌入，并使用内积预测用户的偏好[23]。受深度神经网络表达能力的激励，现代推荐系统通过深度学习技术进一步改进，以利用用户和项目之间更复杂和非线性的特征交互[48]。一些代表性模型包括Wide&Deep[6]、NCF[18]、DeepFM[15]、xDeepFM[26]、CDAE[27]等。然而，这些基于CF的模型通常被设计为近似一阶相似度，例如用户和物品之间的直接连接。通过将用户-项目交互重新视为二分图，基于图的模型能够探索节点之间的隐式高阶相似度，这有助于发现个性化推荐系统中用户和项目之间的更深层联系[14，45，50]。 

2.2 Graph Convolutional Networks

图卷积网络（GCN）作为用于结构化数据的卷积神经网络的一种特殊实例，因其在图嵌入中的出色性能而受到了大量关注[16，22，44]。最近，研究人员在网络规模的推荐系统中部署了GCN[3，11，17，30，42，47，50]。例如，GC-MC[3]和RMGCNN[30]将推荐系统框架为矩阵补全，并在用户项二分图上设计GCN。SpectralCF[50]开发了频谱卷积，以识别频谱域中用户和项目之间的所有可能关联。PinSage[47]将有效的随机游走和图卷积相结合，以在Pinterest中生成项目嵌入。GraphRec[11]提出了一种用于社交推荐的异构图卷积网络。NGCF[42]提出了一种嵌入传播层，以在二部图中获取高阶协同信号。LightGCN[17]简化了GCN的设计，使其更简洁，便于推荐。 

尽管上述方法已被证明在生成用户和项目的嵌入方面非常有效，**但已知GCN由于其递归消息传递模式而对输入图的质量敏感[7，52]。换句话说，用户-项目二分图上的轻微扰动会误导GCN输出错误的预测。**在本研究中，我们旨在学习一种同时获得高质量输入图和GCN参数的方法。 

2.3 Over-fitting and Over-smoothing

开发更深层次的GCN时遇到的两个主要障碍是过拟合和过平滑[24，25，29]。**过拟合来自于使用过度参数化的GCN来拟合给定有限训练数据的分布的情况。**相反，**过平滑导致图节点的特征在增加卷积层的数量时逐渐收敛到相同的值[25]**。以上两个问题都可以通过在GCN中使用 dropout 技巧来缓解。例如，vanilla Dropout[38]随机 mask 权重矩阵中的元素，以减少过拟合的影响。然而，**Dropout并不能防止过度平滑，因为它不会改变图的邻接矩阵**。DropNode[16]是一种面向节点的方法，它随机选择用于小批量训练的节点。DropEdge[35]是一种面向边缘的方法，它从输入图中随机删除一定数量的边缘，就像数据增强器一样。Message dropout[42]随机丢弃每个传播层中的传出消息以细化表示。DropoutNet[40]在训练期间应用输入dropout，以解决推荐系统中的冷启动问题。尽管如此，这些丢弃技术**通常会随机删除某一部分节点、边或特征，这可能导致性能次优**。 

我们的随机二元掩码的机制与上述丢弃方法略有不同，但与图稀疏化的最新发展更为相关[13，49]。我们引入了一种优化算法，作为随机抽样的替代方案，以确定**以数据驱动的方式删除哪条边**。因此，最能保持所需属性（例如，稀疏和低秩）的稀疏图可以在更好的鲁棒性和更好的泛化方面使GCN受益。 

3 THE PROPOSED MODEL

在本节中，我们将详细介绍拟议的SGCN模型。我们的SGCN主要由三个部分组成：精心设计的GCN、随机二进制掩码和秩近似。最后，我们引入了用于模型优化的损失函数。 

3.1 Problem Formulation

在本文中，我们专注于从隐式反馈中学习用户的偏好。具体来说，点击、评论、购买等行为数据涉及一组用户U = {U}和项目I = {I}，其中$I^+_U$表示用户U之前交互过的项目，$I^−_U$ = I−$I^+_U$表示未观察到的项目。未被观察到的交互并不一定是负面的，用户可能只是没有意识到它们。

当把用户-物品交互看成二部图G时，我们可以构造一个隐式反馈矩阵R∈R|U |×|I |，其中|U|和|I|分别表示用户和物品的数量。每个条目Ru，如果用户u与项目i有交互，i = 1，否则Ru,i = 0。可得其对应的二部图邻接矩阵A为:

![image-20221121150458405](https://gitee.com/ning13445/picture/raw/master/1/image-20221121150458405.png)

其中邻接矩阵A可以用作后面GCNs的输入图。我们的目标是推荐用户u∈U感兴趣的 $I^−_U$ 项目的排序列表。

3.2 GCN for Recommendation

3.2.1 Embedding Layer.

遵循主流的图卷积推荐系统[17，18，42]，我们描述了用户的表示 u 和一个项目 i 通过嵌入查找表： 

![image-20221121150525342](https://gitee.com/ning13445/picture/raw/master/1/image-20221121150525342.png)

其中 u 和 i 表示用户和项目的id; eu∈Rd和ei∈Rd分别为用户u和物品i的嵌入，d为嵌入大小。这些嵌入被期望记录项目和用户的初始特征。接下来我们介绍两个最先进的基于GCN的推荐模型。

3.2.2 NGCF. 

遵循标准GCN[22]，NGCF[42]利用用户项二分图执行嵌入传播和特征变换，如下所示： 

![image-20221121150542840](https://gitee.com/ning13445/picture/raw/master/1/image-20221121150542840.png)

式中e(k)u和e(k)i，初始化如式(2)中e(0)u = eu和e(0) i = ei，分别表示GCN第k层用户u和物品i的精确表示;  $\sigma$(·)为非线性激活函数，⊙为元素积; W1和W2是可训练权重矩阵; Nu表示与用户u直接交互的项目集合，Ni表示与项目 i 连接的用户集合。随着叠加更多卷积层，模型能够探索用户与项目之间的高阶协同信号。

3.2.3 LightGCN. 

几项研究指出，更简单、有时线性的GCN对于表征学习非常有效[44]。最近，LightGCN[17]旨在简化NGCF的设计，使其更简洁，便于推荐。 

与NGCF相比，LightGCN采用加权和聚合器，并放弃使用特征变换和非线性激活。因此，等式（3）中的传播可以简化为： 

![image-20221121150634375](https://gitee.com/ning13445/picture/raw/master/1/image-20221121150634375.png)

上面的等式可以用紧凑矩阵形式重写。设第0层嵌入矩阵为$\mathbf{E}^{(0)} \in \mathbb{R}^{(|\mathcal{U}|+|I|) \times d}$, 其从等式（2）收集用户和项目的所有嵌入。然后，我们可以得到等式（4）的等价矩阵形式： 

![image-20221121150650286](https://gitee.com/ning13445/picture/raw/master/1/image-20221121150650286.png)

其中![image-20221121150803272](https://gitee.com/ning13445/picture/raw/master/1/image-20221121150803272.png)；D是对应的对角度矩阵，其中每个元素Di,i 表示矩阵 A 的第 i 行非零的个数。 

3.2.4 Model Optimization for NGCF and LightGCN.

通过传播 K 层，GCNs 获得 K + 1 个嵌入来表示一个用户(e(0)u，…， e(K)u)和一个项(e(0)i，…， e(K)i)。使用一个聚合函数来获得最终的表示形式:

![image-20221121150826609](https://gitee.com/ning13445/picture/raw/master/1/image-20221121150826609.png)

NGCF通过串联实现AGG（·），而LightGCN使用加权和。内积用于预测偏好得分： 

![image-20221121150840806](https://gitee.com/ning13445/picture/raw/master/1/image-20221121150840806.png)

两种方法都使用贝叶斯个性化排序（BPR）损失[34]来优化模型参数，即最小化： 

![image-20221121150910944](https://gitee.com/ning13445/picture/raw/master/1/image-20221121150910944.png)

其中O为成对训练数据;A(·)为sigmoid函数;Θ表示模型参数，a控制L2范数以防止过拟合。

3.2.5 Limitations.

尽管NGCF和LightGCN取得了成功，但我们认为它们不足以解决二分图中的噪声。例如，LightGCN完全依赖于邻接矩阵A来细化等式（5）中的用户和项目的表示。然而，如第1节所述，矩阵A可能包含噪声边，这些误导性信息随着LightGCN的深入而继续传播。当图形噪声信号包含低频分量时，情况变得更糟。因此，GCN存在过度适应噪声的高风险[33]。 

另一方面，vanilla Dropout[38]随机屏蔽了权重矩阵的元素（例如，等式（3）中的W1和W2），这可能具有有限的防止噪声的能力，因为它不会对相邻矩阵A进行任何更改。然而，这削弱了在训练阶段应保留或删除哪条边的可解释性和理解性（详见第4.3节）。为了应对这一挑战，我们提出了一种简单而有效的数据驱动原理，即随机采样的替代方案，通过使用随机二进制掩码来屏蔽边缘。 

3.3 Stochastic Binary Masks

3.3.1 Graph Sparsification. 

![image-20221121150928877](https://gitee.com/ning13445/picture/raw/master/1/image-20221121150928877.png)

为了滤除噪声，我们将GCN的每一层附加一个随机二进制掩码，以在训练GCN参数的同时修剪不重要的边。整个网络架构如图2所示。形式上，对于等式（5）中的每个卷积层，引入二元矩阵Z(k)∈{0,1}，其中Z(k)u,v表示节点u和节点v之间的边是否包含在第k层中。与Eq.(5)中固定的邻接矩阵不同，第k层的输入图邻接矩阵为:

![image-20221121150940916](https://gitee.com/ning13445/picture/raw/master/1/image-20221121150940916.png)

⊙ 表示元素乘积。直观地说，随机二进制掩码Z(k) （即，1被采样并且0被丢弃）可以被视为图生成器，以便支持GCN的每一层的高质量稀疏图。稀疏图在训练期间支持邻居聚集的**子集**，而不是完全聚集，从而避免了GCN深入时的**过平滑**。这种图稀疏化的想法并不新鲜。事实上，它的原始目标是在保留输入图的基本信息的同时去除不必要的边以进行图压缩[10]，最近在深度图模型[13，49]中重新讨论了这一点。 

为了鼓励A(k)的稀疏性，使用L0正则化显式地惩罚Z(k)的非零项个数，通过最小化:

![image-20221121151008728](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151008728.png)

其中∥·∥0表示L0范数，可以使不重要的边完全为零。I[c]是一个指示函数，如果条件c成立，则该函数为1，否则为0。然而，由于二元掩码Z(k)的 $2^{| G |}$ 个可能状态的不可微性、离散性和组合性，这种惩罚下的优化在计算上是难以处理的。 为了应对这一挑战，我们将这些离散变量重新参数化为基础连续变量的确定性变换，然后应用反向采样来产生低方差和无偏梯度。接下来，我们将介绍一种有效的算法，以更好地通过随机二进制层反向传播梯度。 

3.3.2 Reparameterization and Gradients.

由于Z(k)与原始GCNs(如NGCF或LightGCN)共同优化，我们将Eq.(6)和Eq.(8)合并为一个统一目标:

![image-20221121151036532](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151036532.png)

其中 $\beta$ 控制图的稀疏性。因此，式(9)涉及到随机梯度估计，需要对 $2^{| G |}$  个二元序列进行边缘化。为此，我们认为每个Z(k)u,v服从参数Π(k)u,v∈[0,1]的伯努利分布，使Z(k)u,v∼Bern(Π(k)u,v)。式(9)可以重新表述为

![image-20221121151054351](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151054351.png)

其中 E 为期望，而Eq.(10)中的目标 L 实际上是Eq.(9)中目标 L 在参数 Π(k) 上的变分上界。现在第二项关于新的参数Π(k)是可微的，而第一项由于Z(k)的离散性质仍然是有问题的。

> Marginalization边缘化，为了将一些不必要计算的变量省去，而采用的方法
>
> 进行一次伯努利试验，成功(X=1)概率为p(0<=p<=1)，失败(X=0)概率为1-p，则称随机变量X服从伯努利分布。

为了有效地计算梯度，我们使用重新参数化的方法 技巧[20]，将Π(k)u,v∈[0, 1]重新参数化为参数 $\phi$(k)u,v 的确定性函数g(-)，因此

![image-20221118092944097](https://gitee.com/ning13445/picture/raw/master/ning13445/picture/1/2022-11-20 12-07-23_image-20221118092944097.png)

因为确定性函数 g(-) 应在[0，1]内有界，因此 sigmoid 函数是一个很好的候选函数：![image-20221121151156098](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151156098.png). 此外，我们还采用了最近提出的无偏梯度估计器（ARM）来解决随机二元优化问题[[46]。我们首先介绍如下关键定理： 

定理3.1（ARM）。对于N个二元随机变量的向量 $z=(z1, . . , zN )^T$，以及任何函数f，其梯度为

![image-20221121151236559](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151236559.png)

关于![image-20221121151336300](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151336300.png) , 伯努利概率参数的逻辑可以表示为： 

![image-20221121151300308](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151300308.png)

由于期望的线性，ARM能够直接优化伯努利变量而不引入任何偏差，这产生了一个极具竞争力的估计器。此外，可以仅使用一对反向耦合的样本来估计期望值，从而能够有效地计算梯度。 

根据上述定理，让f（-）为BPR损失函数：f（Z）=L BPR（Z，Θ），以及重新参数化：![image-20221121151449988](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151449988.png)，我们现在可以用以下矩阵形式计算公式（10）中ˆL的梯度，即Φ。

![image-20221121151410878](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151410878.png)

其中f（I[U>$\sigma$（-Φ）]）是在GCN的前向传递中，如果U（k）>$\sigma$（-Φ（k）），将二进制掩码Z（k）设置为1，否则为0而得到的BPR损失。同样的策略也适用于f(I[U < $\sigma$ (Φ)])。

为此，我们可以通过随机二元掩码有效地对梯度进行反向传播，因为：1）从伯努利分布的采样简单地被从0到1之间的均匀分布的采样所代替；2） 等式（11）的第一项仅涉及GCN的前向传播以计算梯度；3） 第二项∇Φ$\sigma$(Φ）是可微的且易于计算。这些特性非常吸引人，这意味着我们可以计算从离散空间到连续空间的梯度。 

在推理阶段，我们可以用Z(k)uv~Bern(Π(k)u,v )的期望值作为公式(7)的掩码，即![image-20221121151531611](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151531611.png).然而，这不会产生一个稀疏图A(k)，因为定理3.1中的sigmoid函数是平滑的，而且掩码的元素没有一个是精确的零（除非使用硬sigmoid函数）。 在这里，我们只是将那些g(Φ(k)u,v )≤0.5的值剪切为零，这样就可以保证得到一个稀疏图，相应的噪声边也被有效消除。

**Discussion:**

值得一提的是，最近有几项研究被提出用于估计方程（10）中离散变量的梯度，如REINFORCE [43], Gumbel-Softmax [20], Straight Through Estimator [2], 和Hard Concrete Estimator [28].。然而，这些方法要么存在偏差梯度，要么存在高方差，而ARM估计器是无偏差的，表现出低方差，并且具有低计算复杂性，如[46]所示。

等式（11）的ARM估计器非常简单，但需要两次GCN的前向传播来计算BPR损失。在原始论文[46]中，作者还介绍了其变体，即Augment-Reinforce（AR），以克服双前向传播的问题，但这导致了更高的方差。幸运的是，与卷积神经网络（CNN）不同，GCN的层数通常非常少（例如，NGCF和LightGCN的K≤4），双前向传递的复杂性是可以接受的。因此，我们在实验中坚持使用标准ARM。我们知道，可以结合其他先进技术来进一步改进我们的随机二进制掩码的训练，例如DisARM[8]。我们将此扩展留在将来。 

3.4 Low-rank Approximation

除了通过二进制掩码实现稀疏图之外，GCN本身还存在针对小扰动的漏洞[7]。对一个节点的更改可能会影响同一本地社区中的其他节点。最近，一些研究表明，具有低秩约束的图对扰动更为鲁棒[9，21]。在这项工作中，我们进一步对邻接矩阵A(k)施加低秩约束, 0≤ k ≤K, 通过最小化： 

![image-20221121151553913](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151553913.png)

其中，∥-∥∗表示核规范，它是秩最小化的凸代名词。$\lambda$i (A(k))表示 A(k) 的第 i 个最大奇异值。通常需要进行奇异值分解（SVD）来优化核规范[9]。

SVD可以很容易地实现，但在反向传播过程中，SVD通常在数值上不稳定[19，41]。这是因为核范数的偏导数依赖于具有元素[19]的矩阵K： 

![image-20221121151619981](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151619981.png)

当两个奇异值接近时，偏导数变得非常大，导致算术溢出。对于大矩阵尤其如此，其中两个奇异值几乎相等的可能性比小矩阵高得多。幂迭代（PI）方法是解决这个问题的一种方法。PI依赖于迭代过程来逼近主要特征值和特征向量。尽管如此，PI对如何在每个放气步骤开始时初始化奇异向量很敏感[41]。 

为了解决这些问题，我们把注意力转向最近的算法，该算法友好地结合了SVD和PI[41]。对于核规范，顶部的奇异值信息量更大，因此我们采用截断的SVD来近似公式(12)为![image-20221121151718021](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151718021.png)。正如[41]所建议的，混合策略如下。1）在前向传递中，我们使用截断的SVD来计算每个邻接矩阵的![image-20221121151739784](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151739784.png)，并基于Λ(k)计算核规范。2) 在反向传播中，我们从PI推导中计算梯度，但使用SVD计算的向量V(k)作为初始化目的。整个 计算图在图3中显示。

![image-20221121151854936](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151854936.png)

​                                 公式（12）中的核规范损失Rl的计算图。影子损失ˆRl只在反向传播中使用。

总之，SVD不参与反向传播（routine 2），SVD只参与前向传递，计算公式（12）的核规范损失Rl（routine 1），并初始化PI的状态。相比之下，PI不参与前向传递（routine 1），它只用于计算反向传播过程中的梯度（routine 2）。结果计算图在数值上是稳定的，并且可以在GCN中施加低秩约束。

3.5 Joint Training

3.5.1混合损失。为此，我们共同学习了推荐任务的图结构和GCN模型。结合等式（6）、等式（8）和等式（12）中的损失，SGCN的总体目标函数如下： 

![image-20221121151818028](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151818028.png)

$\beta$ 和 $\gamma$ 是分别控制稀疏度和低秩约束的超参数。SGCN的总体训练总结在算法1中。 

3.5.2 Model Complexity.

SGCN的复杂性来自三个部分：基本GCN（NGCF或LightGCN）、随机二进制掩码和低秩约束。基本GCN具有与NGCF或LightGCN相同的复杂性，表示为O(T ). 随机二进制掩码的复杂性主要来自等式（11）中的ARM，这需要GCN的两次正向传递。如前所述，GCN中的层数通常很少。因此，ARM的复杂性大致为O（2T ), 这比标准梯度反向传播便宜得多[46]。此外，低秩约束的主要复杂性是SVD计算。最近，有人提出了一些突破k-SVD，如Block Krylov方法[31]或LazySVD[1]。作为矩阵A(k) 自然稀疏，只需要O（nnz（A(k)) 来计算前n个奇异值和它们相应的奇异向量。尽管SGCN包含了稀疏和低秩信息，但计算复杂度仍然与最先进的推荐GCN相同。 



**MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems**

ABSTRACT

图形神经网络（GNN）最近成为最先进的协作过滤（CF）解决方案。CF的一个基本挑战是从隐式反馈中提取负信号，但基于GNN的CF中的负采样在很大程度上尚未被探索。在这项工作中，我们建议通过利用用户-物品图结构和GNN的聚合过程来研究负采样。

我们提出了MixGCF方法，这是一个通用的负采样插件，可以直接用于训练基于GNN的推荐系统。在MixGCF中，我们设计了hop mixing技术来合成困难负样本，而不是从数据中采样原始负样本。具体来说，hop mixing的思想是通过**聚合来自原始负样本邻域不同层的嵌入来生成合成负样本**。通过理论支持的困难选择策略优化 层和邻域 选择过程。

1 INTRODUCTION

推荐系统已被广泛用于避免信息过载的应用，如网上购物[53]、社交网络[29]、广告[15]和网络搜索[17]。它的目标是为用户提供个性化的信息反馈，也就是说，对于每个用户，推荐的问题是预测她或他将消费的物品。

解决这一问题的最有前途的技术之一是使用协同过滤（CF）[21]，该技术对用户与物品的历史交互进行建模，以分析用户和物品，以预测未来的交互。迄今为止，最流行的CF解决方案是将用户和物品投影到潜在嵌入空间中，例如矩阵分解[23]和基于神经网络[14]的技术。为了进一步提高嵌入质量，一个突出的方向是将用户-物品交互建模为图形，并利用图形神经网络（GNN）[8，11，20]将结构信息纳入嵌入。值得注意的是，基于GNN的推荐模型，如PinSage[49]、NGCF[43]和LightGCN[13]，在Web规模的应用中产生了最先进的性能。 

（基于GNN的）推荐系统的典型流程相对简单。给定一个用户-物品交互图，它首先在结构上定义一个聚合函数来传播邻域信息，然后通常是一个用于输出用户和物品嵌入的池操作。与传统的推荐方法类似，其目标函数被设计为**更倾向于观察到的用户-物品对（作为正的）**而不是未观察到的（作为负的）。以广泛采用的BPR损失[31]为例，对于每个用户和她的一个正样本物品，我们进行负采样，从她从未接触过的物品中选择一个物品作为负样本物品。 

本质上，负样本对（基于GNN的）推荐模型的性能起决定性作用。通常，均匀分布用于负采样[13，43]。为了提高负样本的质量，有研究试图设计新的采样分布，以优先处理优质的负样本[17, 28, 40, 44, 48, 52]。在这样做时，模型将受到挑战，并**被迫以更精细的粒度区分它们的差异。**为了改善GNN中的负采样，PinSage[49]根据其PageRank得分进行负采样，MCNS[48]根据其结构相关性重新设计了正采样和负采样分布。然而，**GNN中的这些尝试只关注于改进离散图空间中的负采样，而忽略了GNN在嵌入空间中的独特邻域聚集过程** 

**Contributions.** 

在这项工作中，我们建议设计负采样策略，以更好地训练基于GNN的推荐系统。我们提出了一个用于生成困难负样本的简单MixGCF框架。MixGCF从数据增强和度量学习中获得灵感，通过利用底层的基于gnn的推荐来合成负样本。而不是直接从数据中抽取真实的负样本样本。 

![image-20221121151939208](https://gitee.com/ning13445/picture/raw/master/1/image-20221121151939208.png)

为了得到困难负样本，MixGCF设计了两种策略：positive mixing 和 hop mixing。在positive mixing中，我们引入了一种插值混合方法，通过将正样本中的信息注入到原始负样本中来污染原始负样本的嵌入。

在hop mixing中，我们对图1中的几个原始负样本(例如![image-20221121152030210](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152030210.png))进行采样，并通过使用从其邻居的选定 hop 聚合的污染嵌入生成 合成负样本 $v^−$ 的嵌入。例如，如蓝色圆圈所示，跳0、1和2分别从中![image-20221121152044106](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152044106.png)选择。值得一提的是，从哪个邻域的hop的选择是由一种设计好的困难选择策略指导的。

In summary, our work makes the following contributions:

•引入合成负样本的想法，而不是直接从数据中提取负样本，以改进基于GNN的推荐系统。 

•提出一个通用的MixGCF框架，其中包含positive mixing 和 hop mixing策略，这些策略可以自然地插入到基于GNN的推荐模型中。 

•展示MixGCF为GNN推荐者带来的显著改进，以及它在一系列负样本采样技术上的一贯表现。 

2 PRELIMINARIES AND PROBLEM

在本节中，我们首先回顾了基于图神经网络（GNN）的推荐系统协同过滤（CF）的总体过程。然后，我们介绍了与上述过程的训练相关的研究问题。 

2.1Graph Neural Networks for Recommendation

通常，推荐系统的输入包括一组用户U={u}，物品V={v}，以及用户的隐性反馈![image-20221121152120696](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152120696.png)，其中每一对表示用户u和物品 $v^+$ 之间的交互。我们的目标是估计用户对物品的喜好。

最近，研究表明，基于GNN的CF模型为这项任务提供了很好的结果[13, 43, 49]。其主要思想是根据用户的多multi-hop来衡量用户u对物品v的偏好。具体来说，这些技术产生了目标用户和物品的潜在表征，其内积被用来量化偏好，即，![image-20221121152159009](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152159009.png)。

接下来，我们简要介绍了基于GNN的CF的过程，包括聚合、池化及其负采样优化。 

Aggregation. 

 每个用户和物品都分别与一个初始嵌入eu和ev相关联，作为其表示向量。为了利用来自邻居节点的CF信号，基于GNN的推荐模型应用不同的聚合函数来传播邻居的信息[3, 13, 37, 43, 49]。以Light-GCN为例，其聚合过程为：

![image-20221121152222898](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152222898.png)

其中e(l)u , e(l)v是用户u和物品 v在GNN第 l 层的嵌入，Nu表示与用户u交互的物品集，Nv表示与物品 v 交互的用户集。通过堆叠多个聚合层，每个用户/物品可以从其高阶邻居那里收集信息。为了简单起见，我们在下面的章节中使用 e(l) 作为第l层的嵌入。

Pooling.

不同于节点分类的GNN[11，20]，其中使用了最终层中的表示，基于GNN的CF模型通常采用池化操作来生成用户和物品的最终表示。根据[47]，这有助于避免过平滑，并确定节点子图信息在不同范围内的重要性。 

具体来说，在最后一层L，池化函数被应用于生成最终的用户/物品表示e∗u/e∗v。例如，LightGCN使用基于和的池化。

![image-20221121152255526](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152255526.png)

NGCF使用基于concat的池化： 

![image-20221121152311221](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152311221.png)

Optimization with Negative Sampling.

学习排名的任务是通过假设用户偏好的物品应该排名高于其他物品，向用户提供物品的排名列表。然而，每个用户的排名物品列表通常只能从仅由积极观察组成的隐式反馈中推断出来。一个简单的解决方案是假设用户更喜欢观察到的物品而不是所有未观察到的。由于未观察到的物品（通常在![image-20221121152356618](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152356618.png)），学习目标通常通过负采样简化为BPR损失[31]： 

![image-20221121152333384](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152333384.png)

其中 v+ 和 v- 分别表示正面和负样本的物品，Pu(a>b)表示用户u喜欢物品a而不是b，![image-20221121152427299](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152427299.png)是负样本样本的分布，Θ是模型的参数。大多数推荐方法都考虑了均匀分布的负样本（![image-20221121152434184](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152434184.png)为![image-20221121152454539](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152454539.png)）。[13, 14, 29, 31, 43].

2.2 The Negative Sampling Problem

根据等式（4）中的损失函数，负采样策略在推荐的模型训练中起着关键作用。直观地说，接近正样本的负样本，即**困难负样本，可以使模型更好地学习正和负实例之间的边界**[49]。为此，已经进行了多次尝试来对困难负样本进行采样，以改进一般推荐系统的优化[6，17，30，52]。 

然而，基于GNN的推荐者的负采样在很大程度上仍未探索。值得注意的是，PinSage[49]和MCNS[48]的早期尝试专注于改进离散结构层面的采样分布，在图中寻找更好的困难负样本（原始）节点。在这项工作中，我们提出了一个问题，即我们是否可以基于推荐者的GNN在连续空间中合成困难负样本。 

3 THE MIXGCF METHOD

MixGCF是基于GNN的推荐中负采样的通用算法。它可以直接插入现有的基于GNN的推荐算法，如LightGCN和NGCF。 

MixGCF建议基于GNN的CF推荐模型的图形结构来合成有益的（和假）负样本，而不是从数据中采样真实物品作为负样本[28，31，40，48，52]。具体来说，MixGCF引入了positive mixing和hop mixing技术，通过混合来自不同局部图的信息来合成负样本。 

![image-20221121152531736](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152531736.png)

​                            Figure 2:MixGCF概述，其中e(l)表示节点e的第l层嵌入，e’(l)表示Positive mixing生成的第l层嵌入。

MixGCF的流程如图2所示。在positive mixing中，我们开发了一种插值混合方法，将信息从正样本注入到负样本，从而生成困难负样本候选。在hop mixing中，我们首先利用困难负样本选择策略从上面生成的每个困难负样本中提取独特信息，然后使用池化操作组合提取的不同信息，以创建虚假但信息丰富的负样本。 

3.1 Positive Mixing

回顾一下，在 L层GNN中，对于每个物品 v，我们可以有L+1个v的嵌入，其中每个 $e^{(l)}_v$ 对应于用 l 层（0≤l≤L）聚合的嵌入。

为了伪造图2中的负样本 $v^-_i$ 与它的嵌入 $e_{v-}$，我们首先遵循惯例[17, 49]，选择M个负样本物品组成候选集M，M通常比数据中的物品数量小得多。这M个负样本物品可以形成一个大小为 M×（L+1）的候选负样本嵌入集 E={$e^{（l）}_{vm} $}。

最近的一项研究[17]表明，推荐模型通常在一个输入空间上运行，该输入空间主要由简单的负面信息组成，因此我们建议提高候选负样本的嵌入E的质量。受mixup[18, 51]的启发，我们引入了 positive mixing 的概念，将正样本信息  $e_{v+}$ 注入到 E 的负样本嵌入中。mixup 是一种基于插值的数据增强方法，它强制模型在训练数据之间线性输出。具体而言，对于每个候选负样本嵌入 ![image-20221121152604741](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152604741.png)∈ E、 positive mixing操作形式化为： 

![image-20221121152628947](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152628947.png)

其中![image-20221121152707821](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152707821.png)是对每个 hop l 均匀采样的混合系数。注意，mixup 的混合系数是从β分布Beta(B, B)中采样的，这对模型的泛化能力有很大影响[50]。为了消除这种影响，在我们的positive mixing中，混合系数![image-20221121152715599](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152715599.png) 是从(0, 1)中均匀采样的(关于 $\alpha(l)$ 的经验讨论，请参考第4.3节)。

设E′是候选负样本的增强嵌入集M。Positive mixing通过（1）将正信息注入到负样本中，使得模型更加专注于识别决策边界，以及（2）利用随机混合系数将随机不确定性引入到它们中来增强负样本。 

3.2 Hop Mixing

嵌入![image-20221121152739046](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152739046.png) 在通过Positive mixing增强的候选负样本项中，我们提出了Hop Mixing技术来生成合成负样本项 ![image-20221121152831993](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152831993.png)及其嵌入.![image-20221121152802895](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152802895.png) Hop Mixing的主要思想是利用GNN中的分层（分层）聚合过程。 

具体来说，对于每一层 l（0≤l≤L），我们从![image-20221121152913311](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152913311.png) 中抽取一个候选负样本嵌入![image-20221121152853699](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152853699.png)，它包含M中所有候选负样本物品的第 l 层嵌入。以L=2为例，我们可以从E′中抽取![image-20221121152941088](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152941088.png)。请注意，a、b、c不一定是不同的。

然后，hop mixing 的想法是将所有按层选择的L+1个嵌入结合起来，生成（假）负样本 $v^-$ 的表示。具体来说，该表征是通过池化操作融合所有候选嵌入来合成的。

![image-20221121152958956](https://gitee.com/ning13445/picture/raw/master/1/image-20221121152958956.png)

其中 ![image-20221121153038792](https://gitee.com/ning13445/picture/raw/master/1/image-20221121153038792.png) 表示在第 l 层采样的 vx 的第 l 层嵌入，fpool(-)应用了当前基于GNN的推荐器中使用的相同的池化操作。

hop mixing的基本问题是如何在每一层的E′(l)中有效地抽出候选嵌入![image-20221121153059637](https://gitee.com/ning13445/picture/raw/master/1/image-20221121153059637.png)。 值得注意的是，最近一项关于图表示学习(MCNS)的负采样研究[48]从理论上表明，最佳参数 ![image-20221121153127607](https://gitee.com/ning13445/picture/raw/master/1/image-20221121153127607.png) 的预期风险在预期损失![image-20221121153144839](https://gitee.com/ning13445/picture/raw/master/1/image-20221121153144839.png) 和经验损失![image-20221121153203743](https://gitee.com/ning13445/picture/raw/master/1/image-20221121153203743.png) 之间得到满足。

![image-20221121153230286](https://gitee.com/ning13445/picture/raw/master/1/image-20221121153230286.png)

其中pd(v|u)、pn(v|u)分别表示估计的正样本分布和负样本分布，T是节点对的数量，K是损失中每个用户招募的负样本。这一推导表明，如果pn(v|u)与pd(v|u)成正比，则预期风险只取决于pd(v|u)，而具有高内积分数的用户-物品对之间的交互概率可以被准确估计。

基于上述理论，建议的负采样方法是根据估计的正样本分布选择负采样。在这里，我们应用内积分数来近似正分布，并选择得分最高的候选样本，这也称为困难负选择策略[30，52]。从形式上讲第 l 层被实现为： 

![image-20221121153243782](https://gitee.com/ning13445/picture/raw/master/1/image-20221121153243782.png)

其中 - 是内积运算，fQ(u, l)是一个查询映射，返回与目标用户 u 有关的第 l 跳的嵌入。

等式（8）中的查询取决于用于推荐的GNN的池模块。如第2节所述，基于GNN的建议[13，43]中的主流池化模块可分为基于和的池化操作和基于合并的池化。因此，在目标用户嵌入eu和合成的负样本ev−嵌入之间存在两种内积选项:

![image-20221121153259343](https://gitee.com/ning13445/picture/raw/master/1/image-20221121153259343.png)

为了使公式（8）中的选择过程与GNN推荐器中使用的集合一致，我们分别让fQ(u, l)= eu用于基于总和的池，fQ(u, l)= e(l)u用于基于联合的池。

3.3 Optimization with MixGCF

现在，我们可以使用所提出的MixGCF方法作为负采样方法fS（·），以优化基于GNN的推荐模型的参数。直截了当地说，BPR损失函数可以更新为 

![image-20221121153317645](https://gitee.com/ning13445/picture/raw/master/1/image-20221121153317645.png)

其中$\sigma$(-)是sigmoid函数，O+是正样本反馈的集合。![image-20221121153354242](https://gitee.com/ning13445/picture/raw/master/1/image-20221121153354242.png)表示实例(嵌入)ev- 是由提议的MixGCF方法合成的。

3.4 Discussions on MixGCF

General Plugin.

从等式（9）中观察到，所提出的负采样方法MixGCF可以自然地插入排名损失。此外，MixGCF是一种简单的非参数方法。总之，这些使MixGCF成为改进一组基于GNN的推荐模型的通用技术。 

Data Augmentation.

与先前的工作不同，MixGCF提出了基于跳跃式采样的综合负样本物品的范式。这种方法也可以从数据扩充的角度理解，因为合成实例与现有实例一致但不同[33]。这使得推荐者能够在更复杂的数据上得到训练，从而提高了泛化能力。 

Approximation of Multiple Negatives.

如度量学习[16，32，34]所示，为每次更新在损失函数中招募多个负样本实例可以加快基础模型的收敛，并提供更好的性能。MixGCF不是直接对多个负样本进行采样，而是通过hop mixing自然地提供了它们的低成本近似值，这可能使其受益于上述度量学习结论。 

3.5 Time Complexity

MixGCF的时间成本主要来自两个部分。对于 hop mixing 模块，跳粒度负样本选择方案的计算复杂性为O(MLd), M 是负样本候选集的大小，L 是GNN层的数量，以及d 是嵌入维度。对于Positive mixing模块，线性组合的复杂性为O(MLd), 其等效于Hop Mixing模块。因此，MixGCF的时间复杂度为O(MLd). 


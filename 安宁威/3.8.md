**Leveraging Title-Abstract Attentive Semantics for Paper Recommendation**

Abstract

论文推荐是为用户提供感兴趣的个性化论文的研究课题。然而，现有的大多数方法都将标题和摘要作为学习论文表示的输入，忽略了它们之间的语义关系。在本文中，我们将摘要视为一个句子序列，并提出了一个两级注意神经网络来捕获:

(1)句子中每个单词反映它是否在语义上接近标题中的单词的能力。

(2)摘要中每句话相对于标题的程度，这往往是对摘要文档的一个很好的概括。具体而言，我们提出了长短期记忆(LSTM)网络，重点学习句子的表示，并将门控循环单元(GRU)网络与记忆网络集成，学习交互论文的长期顺序句式，用于用户和项目(论文)建模。我们在两个真实数据集上进行了广泛的实验，并表明我们的方法在准确性方面优于其他最先进的方法。

Introduction

<img src="https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306194137857.png" alt="image-20230306194137857" style="zoom:200%;" />

现有的基于文本的方法大多局限于简单地将论文标题和摘要区别对待，无法把握两者之间的语义关系。在我们看来，我们将标题视为相对于整个摘要文档的结论性和信息性的句子，因此将两者结合起来可以更好地捕捉到特定论文的语义。为了解释，我们尝试从单词和句子两个层面分析一篇论文，如图1所示。具体来说，在词语层面，我们认为标题中的词语是最能反映论文研究主题的指标。因此，在构建论文摘要句的表示时，我们更重视这些信息性词汇。以摘要中的第四句话为例，突出显示的词语“hybrid”、“recommendation”和“systems”比其他词语更具代表性，因为它们提供了更多与标题一致的语义信息。然后，在句子层面的部分，我们注意到摘要中的每个句子都有不同程度的反映论文语义的能力。例如，图1中的第二句话是一个可能出现在许多其他论文中的一般语句。显然，这句话对于把握论文的主题和语义没有什么帮助。相比之下，第四句话是最重要的，因为它阐述了论文的主要思想。

为了解决这些问题，本文提出了一个标题摘要注意语义(TAAS)网络来捕获标题和摘要之间的语义关系，以进行论文推荐。它包括两个注意子网络，即词级注意子网络和句子级注意子网络。我们把摘要看成是一系列的句子，把标题看成是摘要的结论性句子。具体来说，在词级子网络中，我们提出了一个专注的长短期记忆(LSTM)网络，通过考虑一个词(在摘要句中)相对于标题中的词的重要性来学习句子表示。在句子级子网络中，我们将门控循环单元(GRU)网络1与记忆网络无缝集成，以捕获标题与摘要中每个句子之间的关系。通过这种方式，我们将**通过捕获连续的句子模式来构建细粒度的用户偏好**。

The TAAS Model

General Framework

<img src="https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306195240349.png" alt="image-20230306195240349" style="zoom:200%;" />

我们的TAAS模型是一个两级注意推荐神经网络，主要集中在单词级和句子级学习论文标题和摘要之间的语义关系，从而学习细粒度的用户画像。为了便于讨论，我们将引入一些符号。假设用户集U中有N个用户，论文集 i 中有M篇论文。符号ui和 ij 分别表示用户 i 在U中的嵌入向量和项目 j 在 i 中的嵌入向量。

我们的方法的一般框架如图2所示。我们设计了两个注意子网络来学习基于顺序句型的用户偏好向量 mi，以及从标题和摘要的文本信息中获得的 item  内容向量cj。同样采用(Chen et al 2018)提出的组合方法，用户pi 和item qj 的特征表示形式可以表述为:

![image-20230306170912475](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306170912475.png)

其中α∈[0,1]和β∈[0,1]是表示语义嵌入mi和cj重要性的参数。在实验中，我们将对这两个参数进行调整，以研究摘要与标题之间的语义关系的影响。

Ranking Function. 在优化上述语义权重参数后，对于每个用户 i，我们为每个候选项目 j 计算估计的排名分数xij，以生成项目排名列表。取以下两个向量的内积作为排序函数。

![image-20230306170940254](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306170940254.png)

Objective Function. 为了生成排名前k的论文推荐，我们采用BPR训练我们的模型![image-20230306171551742](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306171551742.png)

Model Workflow.

我们模型的工作流程如图2所示。假设用户 i 对论文 j 有偏好，W首先从标题的第 x 个单词中提取词嵌入W·，x，从第y个摘要句的第x个单词中提取词嵌入wy,x。为了简单起见，我们省略了符号j，因为它们都属于同一篇论文。从标题和摘要句中提取的词嵌入将顺序输入到词级注意子网络，该子网络生成标题 t 和摘要句 ay 的特征表示，即摘要的第 y 句。

具体地说，这些词嵌入将用于通过将摘要词与标题词进行比较来计算摘要词的相对重要性(关注度)。基本的假设是，如果摘要中的词在语义上与标题中的词接近，那么它们就更重要。

接下来，我们将摘要表示为一组连续的句子:a = {a1, a2，…， an}，其中n是摘要的句子数。我们将摘要句子 A 和标题 t 作为句子级注意子网络的输入，其中考虑了每个摘要句子相对于标题的重要性。子网络的输出包括:(1)论文 j 的内容向量 cj，表示从摘要和标题中学习到的项目特征。(2)用户 i 的偏好向量 mi，表示从顺序句型中学习到的用户特征。因此，我们可以分别由式1和式2得到用户和物品的表示。最后，我们的TAAS模型可以通过公式4优化目标函数来训练。

Word-Level Attentive Sub-Network

**该子网络旨在捕捉每个摘要句中的单词与标题中的单词之间的语义关系**，从而提供标题感知的句子表示。我们采用预训练的Word2Vec模型检索标题和摘要中所有单词的嵌入向量。单词在句子中的顺序位置是学习句子表示的重要特征。因此，我们设计了一个LSTM网络来对单词序列进行建模。具体来说，LSTM网络包含一个重复的模块链，每个模块中都有一个单元状态，用于存储重要的顺序信息，这些信息将在每个时间步骤中更新。此外，它能够容纳变长度序列，这适用于我们的标题和摘要的词序列。我们将标题词的集合表示为W = {W·，1,W·，2，…， w·，n}，以及在第y句中摘要词的集合Wy = {Wy,1, Wy,2，…， wy,m}，，其中n和m分别为标题和摘要句的长度。

这些词嵌入随后被输入到两个独立的LSTM网络(但共享相同的权重θ)，该网络将根据之前的状态ht−1和新的输入词向量更新当前的隐藏状态向量ht。

![image-20230306171651334](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306171651334.png)

![image-20230306202821595](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306202821595.png)表示两个LSTM网络在时间步![image-20230306202912526](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306202912526.png)的输出。训练网络学习网络参数θ。

为了有效地掌握标题词与摘要词之间的相关性，我们设计了一种注意机制来学习每个句子词的相对重要性，从而更准确地表示摘要句的嵌入。具体来说，对于摘要句中的每个词，我们通过内积计算其嵌入与每个标题词嵌入之间的相关性，得出累积的相似度分数![image-20230306203521292](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306203521292.png):

![image-20230306171710239](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306171710239.png)

![image-20230306203558030](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306203558030.png)表示标题词序列，n为标题长度。一般来说，分数越高表明句子中单词与标题的相关性越强。然后，我们可以通过一个softmax函数对相似度评分进行归一化，得到摘要句子中每个单词的权重，该函数为:

![image-20230306171725927](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306171725927.png)

其中m为摘要句的长度。然后，我们可以将句子中所有单词的隐藏状态进行聚合，得到其特征表示ay，由:

![image-20230306171740007](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306171740007.png)

注意，标题表示可以通过LSTM网络hn的最终状态来检索。

![image-20230306171917383](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306171757903.png)

最后，我们将标题 t 和摘要句 A 作为句子级注意子网络的输入。

Sentence-Level Attentive Sub-Network

**该子网络的目标是掌握每个摘要句子相对于标题的重要性**，正确地构建细粒度的用户偏好和论文表示。我们认为标题是最能概括一篇论文的句子，而摘要的句子则提供了丰富的语义信息，可以依次说明标题的详细含义或暗示。具体地说，我们引入了一个GRU网络，其中标题表示 t 用于初始化global memory。我们认为，仔细初始化网络权重是很重要的，因此基于rnn的网络可以更容易地训练并获得更好的性能。

然后，当时间步长ts出现新的摘要句时，我们继续更新global memory。

![image-20230306171757903](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306171829680.png)

其中ots表示GRU网络在时间步ts的隐藏状态，¯θ是要学习的网络参数。

尽管GRU网络可以在一定程度上捕捉标题和摘要句之间的语义相关性，但它仍然存在两个可能的缺点。首先，它考虑了同一潜空间中不同摘要句与标题之间的语义关系，从而忽略了摘要句在语义上的潜在差异。其次，随着连续句子的不断到达，**GRU网络可能会丢弃用户长期的偏好，只保留最近的记忆。**为了解决这些问题，受(Chen等人2018)的启发，我们将GRU网络与Key-Value 记忆网络(KV-MN)集成在一起，从中可以推导出用户向量来反映长期偏好。具体来说，KV-MN将每个内存槽分解为一个键向量和一个值向量。KV-MN的一个优点是我们可以将多个潜在的key向量与其值向量相关联，其中==每个key表示论文的一个语义方面==。因此，给定key向量，我们可以相应地读取并合并它们的值向量，以构建用户偏好。

假设我们的KV-MN中有K个 latent keys，整个网络共享 latent keys 集中的 K 个嵌入 key 向量F = {f1, f2，…fk}。对于==用户i，我们将她对应的K个内存值向量定义为Vi = {vi1, vi2，…vik}。==注意，值向量因人而异。KV-MN的输入是GRU网络的隐藏状态，表示在某一时间步累积的关于顺序句型的知识。KV-MN的记忆过程主要包括两个操作，即读和写，如图3所示。

> 记忆网络，利用记忆组件保存场景信息，以实现长期记忆的功能。对于很多神经网络模型，RNN，lstm和其变种gru使用了一定的记忆机制，在Memory Networks的作者看来，这些记忆都太小了。

<img src="https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230307153758671.png" alt="image-20230307153758671" style="zoom:200%;" />

Reading. 

在读取操作中，我们利用多个潜在的 key 向量来匹配它们自己的值向量，然后根据它们对用户偏好的重要性权重将它们相加。具体来说，我们首先定义时间步长 t 中关键特征 fk 的注意力得分rk(t)，为:

![image-20230306171829680](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306171917383.png)

然后我们可以通过一个softmax函数计算每个语义空间相对于用户偏好的重要性:

![image-20230306172044087](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306171931663.png)

其中γ是要调优的强度参数。现在，我们可以通过将值向量与上一个时间步骤tr中每个语义空间zk(tr)的重要性权重相加得到用户偏好向量mi，定义为:

![image-20230306171931663](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306171942838.png)

Writing. 

**用户 i 的值向量**将在当前时间步长tc更新，以记住论文的有效语义信息，并忘记无用的记忆。记忆遗忘的比率g计算如下:

![image-20230306172016446](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306171959767.png)

其中E和bg是要研究的遗忘参数。因此，我们通过考虑每个语义空间zk(tc)的重要性来更新值向量:

![image-20230306171959767](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306172016446.png)

![image-20230306210522514](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306210522514.png)指示按元素计算的乘积。之后，通过取归一化函数计算内存向量，然后更新用户的值向量:

![image-20230306171942838](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230306172044087.png)

其中H和by是要学习的内存参数。直观地看，语义空间越重要，对对应的值向量的影响就越大。

**GRU网络的输出向量表示 item 内容向量，即 cj**。用户偏好向量mi可以通过在最后一个时间步对隐藏状态的输出应用读取操作来获得。最后，将基于内容的嵌入mi和cj作为成对排序目标函数的输入，进行模型训练。

Experiments

Datasets. 

我们的实验采用了两个真实的数据集，即citeulike-a和PRSDataset。这两个数据集都包含论文的标题和摘要信息，以及用户与论文之间的交互信息。对于每个数据集，我们随机将其分为两个子集，其中80%的用户论文交互被归类为训练集，其余20%被归类为测试集。

第一个数据集citeulike-a来自CiteULike，另一个数据集PRSDataset来自CSPubGuru。

对于这两个数据集，我们删除了缺少和有缺陷的摘要项以及它们之间的相互作用。我们还过滤掉与最多一个项目有交互的用户。最后，citeulike-a数据集由5548个用户、10987个项目(论文)和134510个用户-项目对组成。

PRSDataset数据集由2453个用户、21940个条目和35969个用户-条目对组成。采用基本文本处理，去除标题和摘要中的停顿词，以及20个字符以下的分段(摘要)句。

Parameter Settings.最佳模型参数可以由文献提出，也可以由实验经验确定。我们在{128,256,512,1024}中测试了神经批处理大小，在{0.1,0.01,0.001}中测试了L2损失权重，并将语义权重参数α和β从0调整为1步进，将学习率调整为{0.1,0.01,0.001,0.0001}。此外，我们测试每个隐藏层中的神经元数量和潜在记忆键的数量，从10到100步进10。所有网络参数均按正态分布(0,0.1)初始化。我们用亚当梯度下降法优化我们的模型。





**SHARE: Designing multiple criteria-based personalized research paper recommendation system**

SHARE提供的推荐流程描述如下。

Input：用特征表示的论文语料库。

Output： top5推荐。

Step 1首先用户需要注册自己，提供基本信息才能使用SHARE。注册后，系统生成的用户id将用于登录，并存储在系统中以供进一步处理。

Step 2用户需要使用用户id和密码登录，才能获得推荐。

Step 3现在，用户可以提供搜索关键字并要求推荐。

Step 4爬虫根据给定的关键词从论文库中提取出一组论文(候选论文集)。

Step 5下一步将用户id与用户画像数据库中已有的用户id进行匹配。

Step 6如果数据库中不存在用户id，将对候选论文集应用MCDA排名(参见3.6.1)对论文进行排名。从论文排名中设置前5名的论文进行推荐。

Step 7接下来，从结果集中，用户的单击活动将与相应的用户id保存在一个日志文件中。

Step 8从日志文件中创建用户画像。

Step 9另一方面，如果用户id在用户数据库中已经存在，则**用户意图模型根据该特定时间用户感兴趣的主题来预测用户的意图。**

Step 10之后，根据主题组匹配，再次从候选论文集中检索少量论文。

Step 11下一步，本文提出的**混合排名算法**将被应用于论文排名。最后，推荐排名前5位的论文。

Step 12从结果集中，将再次分析用户单击的活动，并相应地更新用户画像。



**Step 6**

对于推荐新用户，采用了多标准决策分析(MCDA)技术，即简单相加权(SAW)。MCDA技术将多个(相互冲突的)标准作为决策过程的一部分进行评估。在现有的MCDA算法中，SAW是一种简单且常用的方法。为了选择标准，这里考虑了论文的四个特征。

关键字多样化度量(KDM):它根据关键字之间的关联程度对论文进行评分。相关性越高，得分越低，反之亦然。KDM的目的是**找出具有不同主题和子主题的论文**。它是在一项促进多样性的建议中提出的。

句子复杂度分析(SCA): SCA根据写作风格的复杂性对论文进行分析，并根据难度等级分配分数。

引文时间分析(CAOT): CAOT计算分析与当前时间相关的流行趋势的论文得分。

科学质量测量(SQM):为了确定论文的科学和学术表现，SQM的使用取决于期刊文章作者的影响因子和h指数。

**Step 9**

算法1:用户意图预测

输入:M个用户的浏览记录。

输出:两个最受欢迎的主题

for each user do

​	准备的用户日志文件，包括论文标题，主题，KDM, SCA, CAOT, SQM，日期，两次点击论文之间的时间，点击顺序。

​	规范化数据

​	重塑数据以形成batch size、时间戳和input size。

​	初始化模型参数

​	应用LSTM来预测用户的下一个期望的两个主题组。

end

**Step 11**

本文提出的排序算法结合了基于多准则的排序和 rankSVM（2000年）两种排序策略，提高了推荐的质量，使推荐更具个性化。对于基于多准则的排序，SAW方法的应用方式与“新用户推荐”一节中讨论的类似。此外，应用rankSVM对基于用户偏好的论文进行排序。**rankSVM将排序问题转化为pairwise的分类问题，然后使用SVM分类模型进行学习并求解**。

**Experiment and experimental results**

在本实验中，使用收集到的数据集中的原始数据来表示文章。将收集到的数据集的用户数据按照80:20的比例随机分为训练数据集和测试数据集。实验进行了10次，每次数据集都使用不同的随机分割。


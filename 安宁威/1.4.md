基于学术知识图谱及主题特征嵌入的论文推荐方法 

![image-20230104200330446](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104200330446.png)

本文提出了一种融合了主题文本特征和学术知识图谱实体关系特征的论文推荐模型KE-EDM，模型框架如图1所示。该模型分为特征表示模块和推荐模块两部分。

在特征表示模块中，模型首先将学术论文知识图谱中的图谱结构数据和文本结构数据在同一特征空间进行向量化表达并融合。其次为研究者建立历史行为特征矩阵并将其输入推荐模块，包括曾经发表和引用的论文向量表示。最后，推荐模块在得到研究者的历史行为特征表示后，在编码端使用Transformer对其中的融合特征向量进行学习，从而获取研究者的偏好，本文预设了一个“研究者-主题文本查询”的论文推荐场景，研究者希望获得与某一主题相关且与自身研究偏好相近的论文，在解码端通过输入个人信息以及所要查询主题的短文本，模型会根据其以往的研究历史从候选论文集中为研究者推荐感兴趣的论文。

3.1 问题定义
（1）学术论文推荐

假设有N个研究者，u1,u2···un∈U，研究者 ui 为推荐模型提供自己所要查询的论文主题信息Tui，模型 F 会根据研究者ui的个性化条件提供一组推荐论文Pui，具体来说，模型可以使用的特征包含研究者的身份信息ui、主题信息Tui、研究者曾经发表的论文Qui和曾经引用的论文Cui。简而言之，模型的实际作用是根据学者的过去发表论文和引用行为以及在主题约束的情况下，预测研究者希望看到的论文Pui，模型公式如式(1)所示。

![image-20230104200818369](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104200818369.png)

（2）学术论文知识图谱

为了更好地使用论文中存在的各类实体之间的关系信息，本文引入多种类型的结点和关系组成的学术论文知识图谱的概念，并给出如下定义：

学术论文知识图谱定义：学术论文知识图谱包含的实体有五类，分别为学术论文P、出版机构V、年份Y，作者Au和工作机构Af(作者工作的科研机构)，这五类实体广泛存在于各大数据库的元数据和论文内容中，易于获得。在具体操作中，可以从每一篇科研论文的文本内容或对应的元数据中提取相应的实体，并将其与当前图谱中的现有实体进行匹配，从而逐步扩大和完善学术论文知识图谱，从而保证了学术论文知识图谱的可成长性。实体之间的关系有六种，分别为作者与学术论文之间的发表关系、作者在相关科研机构的工作关系、学术论文与出版机构之间的刊载关系、学术论文和年份之间的出版时间对应关系以及作者与作者之间合作关系。

3.2 特征表示模块

本文需要进行特征表示的数据分为两类：学术论文知识图谱结构特征和论文的主题文本特征。本小节依据数据模态的不同而使用不同的特征表示方法，将各类型的特征表示进行不同程度的融合，使其嵌入到同一特征空间中作为推荐模型的输入数据。

（1）学术论文知识图谱的向量化表达

学术论文知识图谱构建完成后，通过网络嵌入方法生成各实体和关系的向量化表示。受当前知识图发展的启发，为将不同类型结点和关系在统一特征空间中进行向量化处理，本文选择使用TransD[10]方法对存储在图谱结构中的实体和关系进行向量化处理。这是因为TransD不仅考虑了关系和实体的多样性，其参数相对其他翻译模型大大减少，且没有矩阵向量乘法运算，大大降低了运算的复杂性。更适合于大型图谱数据的演算。

具体来说，假设实体(ℎ,t)∈E(实体集)之间存在关系r∈R(关系集)，h,t,r分别表示ℎ,t,r的向量化表示。TransD为每个命名的符号对象(实体和关系)使用两个向量表示。第一个捕获实体(关系)的含义，另一个用于构造映射矩阵。给定一个三元组(ℎ,t,r)，它的向量可以表示为h,hp,t,tp∈Rn和r,rp∈Rm，其中带有后缀p的向量表示映射向量。因此，映射矩阵𝑴 rh ,

𝑴 rt∈Rm×n 的定义分别为式（2）和式（3） 

![image-20230104202319418](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104202319418.png)

![image-20230104202326739](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104202326739.png)

其中𝑰 𝑛×𝑜 表示单位矩阵。在此基础上，实体的映射定义分别为式（4）和式（5）。

![image-20230104202404403](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104202404403.png)

使用评分函数式（6）来衡量嵌入的三元组向量的性能。

![image-20230104202419436](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104202419436.png)

给定约束||𝒊|| 2 ,||𝒔|| 2 ,||𝒖|| 2≤ 1，嵌入训练的损失函数为式（7） 。

![image-20230104202440135](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104202440135.png)

其中[x]+表示max(0,x)，![image-20230104202528802](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104202528802.png)表示边缘距离的超参数。![image-20230104202544958](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104202544958.png)表示学术论文信息图谱中存在的三元组（ℎ,r,t），![image-20230104202606352](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104202606352.png)训练中随机抽取产生的负样例三元组(ℎ′,r′,t′)。

（2）文本主题特征的向量化表达

文本数据是学术论文的主要组成部分，包括论文的标题、摘要、关键字和正文内容等。学术论文的匹配和检索需要明确的主题特征作为依据。为了能够使文本主题信息能够更好地表达论文的核心意义，同时不破坏语义信息的逻辑连贯性。本文将标题和摘要分别进行特征向量化处理并进行融合，公式为式（8）。

![image-20230104202645019](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104202645019.png)

为了减少文本语义信息的损失，本文选择PV-DBOW来处理处理向量。

PV-DBOW是基于skip-gram神经网络模型进行的拓展。设uw为单词w的输入向量，vd为使用PV-DBOW模型从文档d中生成的向量表示，该模型训练向量vd学习文档d中单词w的概率，概率表示为p(w|d)，在skip-gram神经网络模型中计算公式为式（9）。

![image-20230104202923260](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104202923260.png)

对于每个文档d由一组单词wd1,wd2···wdnd组成，所有的d构成了一组文档矩阵D，PV-DBOW的目标是最大化日志概率，概率公式为式（10）。

![image-20230104203049877](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104203049877.png)

3.3 推荐模块

在论文推荐阶段，通过对特征表示学习模块中的数字化特征向量进行学习来提高为研究者所推荐论文的准确性。论文推荐模型的结构如图1推荐模块所示：其中前两步输入分别为作者已经发表和引用的论文的融合向量，其中每篇论文的融合向量由论文实体的图谱特征向量和主题文本向量构成，共同作为研究者的历史行为特征。在特征学习阶段，模型引入Transformer从历史行为特征中捕获研究者的研究偏好，在输入1、2中分别将研究者的引用行为和写作行为的特征引入Transformer进行学习，Transformer的优势在于不但能够并行计算，其中包含的self-attention机制还能够使结点不只关注当前位置的向量，还能通过位置编码（Position Embedding）获取上下文的语义信息，从而能对研究者感兴趣的内容进行重点关注，其注意力信号的计算如公式（11）所示。

![image-20230104203115579](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104203115579.png)

位置编码会在向量中加入了位置信息，这样Transformer就能区分不同位置的向量，从而在学习的过程中保持原有语义顺序。编码公式如式（12）、（13）所示。

![image-20230104203156467](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104203156467.png)

为了从经Transformer学习之后的向量中选取最具代表性的特征向量，模型中加入了一个全局池化层（GlobalMaxPool1D）将学习之后的向量进行了维度压缩，作为解码器阶段的注意力特征。

输入3位于解码器阶段，包含三种类型的数据，第一个是候选论文的嵌入化向量集，其容量大小一般大于一篇论文的引文数，且小于整个数据集的论文数。这些候选论文包括正例样本和负例样本。正例样本从作者实际引用的论文信息中收集构建，而负例样本则是从正例样本的所在同一出版机构中的论文库中随机选择。其他两种数据分别为作者实体和本篇论文的主题文本信息的嵌入化向量。

在解码器阶段，采用了双向RNN的结构来完成解码任务，其基本单元使用双向GRU来实现。与LSTM不同，GRU的激活是先前激活ℎt−1和候选激活ℎt′之间的线性插值

![image-20230104203404154](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104203404154.png)

其中更新门zt 可以表示为：

![image-20230104203426335](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104203426335.png)

候选激活ℎt′由公式（16）定义：

![image-20230104203503875](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104203503875.png)

重置门rt由公式（17）定义：

![image-20230104203537196](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104203537196.png)

此外，编码器阶段分配的自适应权重被用来预测候选集中的论文。首先，来自第1、2输入层的注意力特征和第3输入层的候选论文集、研究者实体和主题文本表示被集成为一个双向RNN层的输入。然后计算每篇候选论文的分数。接下来归一化操作由全连接层中的一个Softmax层进行分类，模型可以通过对输出的预测概率分布进行排序来得出前N个推荐。在这种情况下，推荐过程可以被视为多项二分类任务，即是否推荐某一候选论文。于是模型采用二元交叉熵（Binary Cross-Entropy）作为损失函数训练模型：

![image-20230104203556131](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230104203556131.png)
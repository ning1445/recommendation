目前有一个疑问，就是如果只是把现在效果比较好的推荐算法用在论文数据集上，没有针对论文数据集本身的特点进行设计的话，可以算是创新吗？



Concept-Aware Denoising Graph Neural Network for Micro-Video Recommendation

ABSTRACT

最近，快手和抖音等微视频分享平台已经成为人们生活的主要信息来源。在本文中，我们提出了一种新的用于微视频推荐的概念感知去噪图神经网络（Conde）。Conde由三个阶段的图形卷积过程组成，以导出用户和微视频表示：预热传播、图形去噪和偏好细化。通过将用户节点与视频节点连接，以及将视频节点与相关概念节点连接，从视频的标题和评论中提取，来构建异构三方图。为了解决图中的噪声信息，我们引入了面向用户的图去噪阶段来提取能够更好地反映用户偏好的子图。 

1 INTRODUCTION 

面对微视频数据流量的激增，缓解信息过载问题的关键技术是推荐系统，该系统旨在根据用户的偏好对项目进行精确排序。过去已经提出了无数种推荐算法，包括基于顺序的推荐[17，18]、基于图的推荐[16，34]和一系列在工业环境中实用的CTR预测模型[7，9，22，23，33，36]。 

然而，微视频共享平台中的以下三个独特特性阻碍了现有推荐技术提供良好性能： 

C1：现存的多模态模型很难在短视频场景下提取视频内容，因为短视频的平台具有连续、大流量的特性。另一方面，短视频中所表达的丰富故事，用户的评论等将有助于理解用户对其最大的偏好；

C2：尽管短视频平台为用户提供了“喜欢”和“评论”按钮来与之互动，大多数用户仍然只是去浏览下一个视频，而不会留下反馈信息。没有明确的用户交互，很难判断用户是否真的喜欢她/他所观看的内容，甚至还会存在许多错误点击的情况。

虽然我们可以利用观看时间来推测用户的偏好，但仍然会有许多错误的正样本，这在很大程度上会影响着推荐算法的效果；

C3：短视频的平均生命周期非常短。在我们对短视频用户行为进行数据分析，观察到短视频在发布后两天，用户互动数量就急剧减少了。此外，大多数用户行为都是由少数网红上传的短视频造成的。

![image-20221208210541846](https://gitee.com/ning13445/picture/raw/master/1/image-20221208210541846.png) 

所有这些特征共同导致微视频的用户交互稀疏，推荐性能较差，尤其是长尾微视频。为此，在本文中，我们提出了一种新的概念感知去噪图神经网络(命名为Conde)来解决上述挑战。在Conde中，我们的目标是在文本信息和图神经网络之间建立一个结合，以支持视频内容提取和用户偏好学习。

如图1所示，与微视频相关的标题和评论可以很好地代表视频内容。因此，为了解释微视频中丰富的语义，我们对微视频的字幕和评论中提到的概念进行了提取。这里，概念定义为命名实体和语义关键短语(例如，“heartwarming”，“classic HK
movie”)。通过将微视频与相关的概念连接起来，将用户与消费的微视频连接起来，我们可以形成一个以概念为骨干，具有丰富语义信息的三元异构图。

不难看出，并不是所有的微视频概念都能平等地反映不同用户的喜好。此外，如前所述，图中用户视频边会有很多假正样本。因此，当我们通过图卷积得到她/他的表示时，我们进一步为每个用户引入个性化的去噪过程。具体地说，Conde利用一个三阶段图卷积过程来导出用户和项目表示。

在预热传播阶段，我们首先进行卷积，将语义概念信息按照聚合顺序:概念→微视频→用户 传播给相关微视频和用户。然后，我们进一步对用户视频边进行另一层卷积，以利用协同信号。

在预热阶段计算出用户和物品表示后，图去噪阶段旨在为给定用户提取子图，其中以宽度优先搜索的方式去除附近有噪声的微视频、概念和其他用户。

预期得到的子图能更精确地反映用户的偏好。最后，在偏好细化阶段，我们再次对子图执行与热身阶段相同的卷积，以导出用户表示。

本文的主要贡献如下： 

•我们充分利用文本信息来支持视频内容提取。微视频中丰富的语义显示了代表用户偏好的巨大潜力。

•我们为图神经网络提出了一个去噪阶段，以帮助推荐系统，尤其是流式推荐系统，去除噪声信息，并更准确地捕捉用户的高度动态偏好。

•据我们所知，这是推荐系统中首次尝试研究具有去噪目的的异构图神经网络。 

2 RELATED WORK

由于我们的工作与微视频推荐、基于图形的推荐以及针对图形和推荐的去噪相关，因此我们主要关注这三方面的现有方法。 

2.1 Micro-Video Recommendation

与电子商务网站中的商品购买不同，微视频包含更多的故事情节。为了实现微视频更好的语义学习，MMGCN[32]将多模态信息纳入协作过滤框架。它通过分别为每个模态构建用户项二分图来捕获模态特定的用户偏好。ALPINE[11]还为推荐任务设计了一个基于图形的顺序网络，它可以更好地模拟用户的多样性和动态兴趣。THACIL[2]提出了一种用于建模短期和长期行为的分层注意力机制。尽管这些努力取得了令人鼓舞的成绩，但它们都需要视觉特征提取。然而，这种沉重的处理可能过于昂贵，无法用于实际应用。 

2.2 Graph-based Recommendation

近年来，图形神经网络在许多需要网络嵌入和结构建模的领域中表现出了优异的性能。在实践中，GNN方法比传统的基于特征的方法具有更大的表达能力。GNN的关键思想是递归地聚合来自本地邻居的信息。图卷积网络[10]是第一个在网络结构上引入卷积运算的工作，它通过邻域聚合从源节点的一跳邻居收集信息，并通过堆叠多个GCN层实现消息传递。图形注意力网络（GAT）[27]将注意力机制引入GNN，这使得模型能够在信息聚合过程中学习不同邻居的重要性。这些解决方案主要针对所有节点或边都是相同类型的同构图。为了处理包含不同类型节点和边的异构图，HAN[31]使用语义关注层扩展了原始GAT模型，以学习不同元路径的重要性。这些工作的主要思想已被广泛用于不同类型的推荐任务。 

具体而言，异构图通常是通过将知识图中的用户、项目和相关实体（例如，边信息）视为节点，将用户的行为（如点击、购买、添加到购物车和其他语义关系）视为边来构建的。基于真实的推荐场景，[1，4，5，35]主要通过通过元路径聚合邻域信息，将推荐框架扩展为异构图建模。GraphSAGE[6]将标准图卷积网络扩展到归纳设置，并利用批量训练和邻域采样来支持大规模图的推荐。AGNN[19]基于用户-项目交互设计属性图，并利用V AE学习冷启动用户/项目的属性分布。PinSAGE[34]结合随机游动和图卷积层来聚合和传播邻域信息。MG-BERT[12]利用项目同构图上的变换器架构来预处理项目表示，包括图结构重建和掩蔽节点特征重建。由于知识图中有丰富的语义信息，许多工作都致力于探索用户和项目之间的辅助联系，从而使推荐更加准确和可解释。RippleNet[28]是一种基于路径的方法，它首先为KG中的实体分配初始嵌入，然后根据用户的历史点击项目从KG中采样ripple集合。它使用注意力网络来模拟用户对采样波纹集的偏好，以表示用户。它的一些扩展[21，24，30]专注于在项目知识图上使用嵌入传播机制。DKN[29]是另一个将知识图表示纳入新闻推荐的代表性作品。 

2.3 Graph Denoising

以任务依赖的方式执行数据去噪是一种很有前途的策略，以减轻噪声信息的不利影响。例如，踢除与任务无关的边已被验证以提高节点分类的性能[13,37]。最近，[20]选择删除不相关的历史记录以获得更好的顺序推荐。[26]通过迭代去除知识图中覆盖的不相关的三元组来进行知识修剪，从而更好地推荐新闻。

3 METHOD

![image-20221204193539886](https://gitee.com/ning13445/picture/raw/master/1/image-20221204193539886.png)

在本节中，我们提出了一种用于微视频推荐的概念感知去噪图神经网络。具体而言，所提出的Conde旨在通过向相关用户和微视频传播概念语义来导出用户和微图像的表示。如图所示，Conde由三个阶段组成：预热传播、图去噪和偏好细化。在下文中，我们描述了每个阶段在Conde中的使用顺序。 

3.1 Warm-Up Propagation

首先，在微视频的概念提取之后，我们形成了一个三方异构图G=（V，E），其中V和E分别是节点集和边缘集。A节点o ∈ V可能是用户 u, 微视频 m 或一个概念 c. 此外，E中有两种边，当用户点击微视频时，它们将用户与微视频连接起来，当从微视频中提取概念时，它将微视频与概念连接起来。图2展示了这个三方异构图的一个示例。由于我们计划将概念信息注入到用户和项目表示中，因此在开始时，我们根据V中的概念执行一系列图卷积操作 

具体而言，给定具有一组**概念近邻Cm**的微视频m，通过聚合其概念近邻的嵌入，可导出==微视频m的隐藏特征向量hm==:![image-20221204194312169](https://gitee.com/ning13445/picture/raw/master/1/image-20221204194312169.png)，其中em为微视频的嵌入向量，ec为概念嵌入向量。考虑到简单和有效性，我们选择图注意网络[27]实例化AGG(·)函数如下:

![image-20221204194342873](https://gitee.com/ning13445/picture/raw/master/1/image-20221204194342873.png)

其中![image-20221204194556456](https://gitee.com/ning13445/picture/raw/master/1/image-20221204194556456.png)为表示概念c重要性的注意权重，![image-20221204194622753](https://gitee.com/ning13445/picture/raw/master/1/image-20221204194622753.png)为LeakyReLU激活，∥为向量拼接操作。期望hm能够对微视频内容的语义信息进行编码。

再进行类似操作，利用用户u的微视频邻居Mu导出==用户u的隐藏偏好向量hu==:![image-20221204201802276](https://gitee.com/ning13445/picture/raw/master/1/image-20221204201802276.png)，其中eu为用户u的嵌入向量，hj为公式1中导出的微视频邻居j的隐藏特征向量。现在很明显，我们首先将语义信息从概念传播到微视频，然后再从微视频传播到用户。

到目前为止，整个传播过程完全是由概念驱动的。注意，并不是所有的微视频都能被它们的概念邻居很好地表达出来。另外，协作信号在这个过程中没有被利用。因此，我们对微视频应用另一种卷积操作，通过聚合它们的用户邻居。具体而言，==将微视频m的隐藏特征向量hm更新为它的用户邻居集==![image-20221204201838755](https://gitee.com/ning13445/picture/raw/master/1/image-20221204201838755.png)。在这个意义上，我们可以将概念语义进一步传播到用户视频二部分，丰富微视频，特别是长尾微视频的特征表示。

3.2 Graph Denoising

很明显，不同的用户会对微视频内容的不同方面感兴趣。此外，不可避免地存在一些嘈杂的概念和虚假点击，以分散用户的有效表示学习。尽管我们在等式2中有一个注意力机制，但无关和嘈杂的信息仍然使学习过程变得复杂。在这里，我们介绍了一种面向用户的去噪过程，以广度优先搜索的方式过滤微视频和概念。 

具体来说，我们采用一个门控循环单元(GRU)作为合成器，首先识别用户u和她的每个微视频邻居之间的相关性，如下所示:

![image-20221204195130401](https://gitee.com/ning13445/picture/raw/master/1/image-20221204195130401.png)

> **不放回抽样**(Sampling without replacement) ：一旦对象被选中，则将其删除。

其中hu作为GRU的初始态，fu,m编码相关信息。然后，利用**不放回抽样**进行邻居去噪，只保留n个微视频邻居:![image-20221204201951940](https://gitee.com/ning13445/picture/raw/master/1/image-20221204201951940.png)。这些微视频有望更精确地传达用户的喜好。具体来说，我们可以利用一个带softmax函数的全连接层，推导出保留每个微视频邻居m的可能性如下:

![image-20221204195310163](https://gitee.com/ning13445/picture/raw/master/1/image-20221204195310163.png)

其中w是用于图去噪的参数向量。注意，去噪过程产生离散选择，而不是注意机制，这些离散选择对于模型学习是不可微分的。因此，我们选择Gumbel Softmax[8]来实例化Den（·）对于可微离散样本生成： 

![image-20221204195337892](https://gitee.com/ning13445/picture/raw/master/1/image-20221204195337892.png)

其中，![image-20221204202047581](https://gitee.com/ning13445/picture/raw/master/1/image-20221204202047581.png)， x 是从(0,1)的均匀分布中采样得到的。![image-20221209115238377](https://gitee.com/ning13445/picture/raw/master/1/image-20221209115238377.png)为温度系数。当![image-20221204202120451](https://gitee.com/ning13445/picture/raw/master/1/image-20221204202120451.png)很小时，式5产生了一个多模态分布。相反，当![image-20221204202126622](https://gitee.com/ning13445/picture/raw/master/1/image-20221204202126622.png)较大时，所得分布几乎等价于均匀分布。

之后，我们通过![image-20221204202219167](https://gitee.com/ning13445/picture/raw/master/1/image-20221204202219167.png)中的每个微视频m继续对用户的两跳邻居进行上述去噪处理。注意，我们只考虑m的邻居的概念:Nu,m = Cm。我们尝试在第二阶段中混合使用用户和概念邻居，结果证明这不如只使用概念节点。我们推测，在GRU中同时放置不同类型的节点可能会使模型学习复杂化，因为异质信息很难融合在一起。

然后，我们仍然在第一跳去噪中使用相同的合成器来计算相关性信息： 

![image-20221204195612628](https://gitee.com/ning13445/picture/raw/master/1/image-20221204195612628.png)

**其中hv是两跳邻居v的表示**，这里使用嵌入ec的概念(即**hv = ec**)。此外，当我们把相邻的用户放在一起考虑时，当v是用户u '时，取而代之的是隐偏好向量hu '(即hv = hu ')。然后对Nu、m进行类似的去噪处理，通过微视频m: ![image-20221204202303714](https://gitee.com/ning13445/picture/raw/master/1/image-20221204202303714.png)和![image-20221204202335099](https://gitee.com/ning13445/picture/raw/master/1/image-20221204202335099.png)得到用户u的n个两跳邻居

我们可以看到，整个去噪过程是从每个用户开始依次执行的。由于基于GRU的排序器是用热身阶段的用户偏好向量初始化的，因此将G中的用户邻域重塑为在子图![image-20221204202409138](https://gitee.com/ning13445/picture/raw/master/1/image-20221204202409138.png)中更精确地表达用户偏好，其中节点为![image-20221204202423461](https://gitee.com/ning13445/picture/raw/master/1/image-20221204202423461.png)，且它们之间在G中对应的边保留。

3.3 Preference Refinement

现在，可以直接根据相应的![image-20221204202449250](https://gitee.com/ning13445/picture/raw/master/1/image-20221204202449250.png)来细化每个用户的隐藏偏好表示。具体来说，对于![image-20221204202523699](https://gitee.com/ning13445/picture/raw/master/1/image-20221204202523699.png)中的每个微视频m，我们首先细化了m：![image-20221204202545340](https://gitee.com/ning13445/picture/raw/master/1/image-20221204202545340.png)的隐藏特征向量hm。在这里，对于hv，我们采用与方程6中相同的设置。然后，我们细化了隐藏的偏好向量![image-20221204202604354](https://gitee.com/ning13445/picture/raw/master/1/image-20221204202604354.png)。最后，一个微视频m w.r.t.的排名得分用户u的计算方法如下：

![image-20221204200250041](https://gitee.com/ning13445/picture/raw/master/1/image-20221204200250041.png)

其中hm 是在微视频m预热阶段导出的隐藏特征向量. 

3.4 Model Optimization

除了图卷积的嵌入向量和参数之外，Conde的关键是以个性化的方式执行图去噪，以增强用户偏好学习。根据[37]中的工作，对于每个用户u，我们可以通过重复k次去噪过程来生成k个子图![image-20221204201026051](https://gitee.com/ning13445/picture/raw/master/1/image-20221204201026051.png)。之后，我们计算每个子图G的交叉熵损失如下：

![image-20221204200531400](https://gitee.com/ning13445/picture/raw/master/1/image-20221204200531400.png)

其中，yu,m是用户u点击的微视频m的 ground truth，yu,m是使用子图![image-20221204201257371](https://gitee.com/ning13445/picture/raw/master/1/image-20221204201257371.png)计算的相应排名得分。最后，总损失函数如下：

![image-20221204200639175](https://gitee.com/ning13445/picture/raw/master/1/image-20221204200639175.png)

哪里∥Θ∥表示模型参数上的L2正则化，![image-20221204201317886](https://gitee.com/ning13445/picture/raw/master/1/image-20221204201317886.png)是相应的系数。

Discussion.

模特训练后，我们按照热身的顺序→ 图形去噪→ 并生成相应的子图![image-20221204201353817](https://gitee.com/ning13445/picture/raw/master/1/image-20221204201353817.png)和用户偏好向量![image-20221204201338647](https://gitee.com/ning13445/picture/raw/master/1/image-20221204201338647.png)然后，我们将这些偏好向量保存在内存中以用于在线服务。无需每次重新运行图去噪和偏好细化，因为我们相信学习的模型在捕捉用户偏好方面是有效的。子图结构的边际变化将引入可忽略的变化。实际上，通过预先计算hm，所有微视频隐藏特征向量都可以在预热传播阶段为每个用户重用。



##### 门控循环单元（gated recurrent unit, GRU)

GRU 是新一代的循环神经网络，与 LSTM 非常相似。与 LSTM 相比，GRU 去除掉了细胞状态，使用隐藏状态来进行信息的传递。它只包含两个门：更新门和重置门。

**更新门**

更新门的作用类似于 LSTM 中的遗忘门和输入门。它决定了要忘记哪些信息以及哪些新信息需要被添加。

**重置门**

重置门用于决定遗忘先前信息的程度。

![img](https://gitee.com/ning13445/picture/raw/master/1/e872c964f1d39ee25ce35c143362488e.png)

#### 基于softmax的采样

这时通常的做法是加上softmax函数，把向量归一化，这样既能计算梯度，同时值的大小还能表示概率的含义（多项分布）。

![image-20221210220022425](https://gitee.com/ning13445/picture/raw/master/1/image-20221210220022425.png)

于是value=[-10,10,15]通过softmax函数后有σ(value)=[0,0.007,0.993]，这样做不会改变动作或者说类别的选取，同时softmax倾向于让最大值的概率显著大于其他值，比如这里15和10经过softmax放缩之后变成了0.993和0.007，这有利于把网络训成一个one-hot输出的形式，这种方式在分类问题中是常用方法。

但这样就不会体现概率的含义了，因为σ(value)=[0,0.007,0.993]与σ(value)=[0.3,0.2,0.5]在类别选取的结果看来没有任何差别，都是选择第三个类别，但是从概率意义上讲差别是巨大的。

经典的采样方法就是用**softmax函数**加上**轮盘赌方法（np.random.choice）**。但这样会有个问题，就是**无法计算梯度**。

#### gumbel_softmax采样

假设如下场景:
模型训练过程中, 网络的输出为p = [0.1, 0.7, 0.2], 三个数值分别为"向左", “向上”, "向右"的概率。 我们的决策可能是y = argmax§, 也即选择"向上"这条决策。但是，这样做会有两个问题:

1.argmax()函数是不可导的。这样网络就无法通过反向传播进行学习。
2.argmax()的选择不具有随机性。同样的输出p选择100次,每次的结果都为"向上"。而按照概率为0.7的含义,100次应该有70次左右的决策结果是选择"向上".

而gumbel_softmax的作用就是解决上述这两个子问题。

为了在y=argmax§中**引入随机性**, 将其修改为y = argmax(log§ + G).G称之为gumbel分布, 它的数学表达式为G=-log(-log( ξ \xi ξ)))。引入该分布的作用是引入了随机性，且该随机性保证了该分布输出i的概率等于pi。

**解决不可导**的方法可以用gumbel_softmax来处理。也即forward阶段，使用argmax操作，暂时不用管后面反向操作;但在反向阶段则使用gumbel_softmax来做bp计算。



HOW ATTENTIVE ARE GRAPH ATTENTION NETWORKS?

ABSTRACT

图注意网络(GATs)是最流行的GNN体系结构之一，被认为是最先进的用图表示学习的体系结构。在GAT中，每个节点都将自己的表示作为查询来处理它的邻居。然而，在本文中，我们表明GAT计算一种非常有限的注意:注意分数的排名是无条件的查询节点。我们正式将这种受限制的注意定义为静态注意，并将其与严格意义上更具表现力的动态注意区分开来。因为GAT使用静态注意力机制，所以有一些简单的图问题是GAT无法表达的:在一个可控问题中，我们表明静态注意力阻碍了GAT甚至无法拟合训练数据。为了消除这一限制，我们通过修改操作顺序引入了一个简单的修复方法，并提出GATv2:一个比GAT更具有表现力的动态图注意变量。



经典 GAT（Graph Attention Networks） 的图注意力网络（利用 masked self-attention 学习边权重）的聚合过程如下所示：

1. 首先对每个节点 hi 用一个共享的线性变换 W 进行特征增强，W 是 MLP，可以增加特征向量的维度，从而增强特征表征能力

2. 计算 i 节点和  j 节点的注意力系数

注意力系数的计算有多种方法，比如计算 i 和  j 的向量内积（向量内积表征一个向量在另一个向量上的投影）、GAT 使用一个单层前馈网络将 i 和  j 的高维特征拼接并将其映射到一个实数上，输出的数作为注意力系数

![image-20221209215158953](https://gitee.com/ning13445/picture/raw/master/1/image-20221209215158953.png)

3. 归一化，将注意力分配到节点 i 的邻居节点集合上，求得归一化之后的注意力系数

   ![image-20221209215238239](https://gitee.com/ning13445/picture/raw/master/1/image-20221209215238239.png)

4. 根据计算好的注意力系数，把特征加权求和

   ![image-20221209215325198](https://gitee.com/ning13445/picture/raw/master/1/image-20221209215325198.png)



**GATv1 存在的问题**

对于一组固定的 keys，如果不同的 query 对这组 keys 进行 attention，得到的注意力系数相对不变，那么这个注意力系数计算函数就是静态的

如下图所示，GATv1 就是静态的，也就是无论 q 怎么变（q0-q9），一直是 k8 对应的 attention 最大，改进版本 GATv2 解决了这个问题

![image-20221209215548372](https://gitee.com/ning13445/picture/raw/master/1/image-20221209215548372.png)



**GATV2**

将 a 移到非线性结果外再进行运算，此外先将query-key pair先进行concatenattion，再使用 W 进行线性变换：

![image-20221209214612491](https://gitee.com/ning13445/picture/raw/master/1/image-20221209214612491.png)
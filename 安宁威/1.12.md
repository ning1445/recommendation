**GCN**

公式

![image-20230111174148332](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230111174148332.png)

图

![image-20230111173019995](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230111173019995.png)

A：邻接矩阵，X：特征值

![image-20230111173041852](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230111173041852.png)

加入对角线，即节点本身![image-20230111173347508](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230111173347508.png)

D为度矩阵，表示节点的度；Dhat表示加入节点本身![image-20230111173618456](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230111173618456.png)

![image-20230111173722425](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230111173722425.png)



**GAT**

公式

相似度分数

![image-20230111211456562](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230111211456562.png)

注意力权重

![image-20230111211054174](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230111211054174.png)

节点新的特征向量

![image-20230111213707213](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230111213707213.png)



代码

    def __init__(self, in_features, out_features, dropout, alpha, concat=True):
        super(GraphAttentionLayer, self).__init__()
        self.in_features = in_features  # 节点表示向量的输入特征维度
        self.out_features = out_features  # 节点表示向量的输出特征维度
        self.dropout = dropout  # dropout参数
        self.alpha = alpha  # leakyrelu激活的参数
        self.concat = concat  # 如果为true, 再进行elu激活
    
        # 定义可训练参数，即论文中的W和a
        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))
        nn.init.xavier_uniform_(self.W.data, gain=1.414)  # xavier初始化
        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))
        nn.init.xavier_uniform_(self.a.data, gain=1.414)  # xavier初始化
    
        # 定义leakyrelu激活函数
        self.leakyrelu = nn.LeakyReLU(self.alpha)
    
    def forward(self, input, adj):
        """
        input: input_fea [N, in_features]  in_features表示节点的输入特征向量元素个数
        adj: 图的邻接矩阵 维度[N, N] 非零即一
        """
        h = torch.mm(input, self.W)  # [N, out_features]
        N = h.size()[0]  # N 图的节点数
    
        #拼接
        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)
        
        # 公式leakyrelu（eij）。[N, N, 2*out_features]
        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))
        # [N, N, 1] => [N, N] 图注意力的相关系数（未归一化）
    
        zero_vec = -1e12 * torch.ones_like(e)  # 将没有连接的边置为负无穷
        
        #torch.where：判断条件
        # 表示如果邻接矩阵元素大于0时，则两个节点有连接，该位置的注意力系数保留，
        # 否则需要mask并置为非常小的值，原因是softmax的时候这个最小值会不考虑。
        attention = torch.where：(adj > 0, e, zero_vec)  # [N, N]
       
        #公式aij。dim=1时， 是对某一维度的列进行softmax运算
        attention = F.softmax(attention, dim=1)  # softmax形状保持不变 [N, N]，得到归一化的注意力权重！
        attention = F.dropout(attention, self.dropout, training=self.training)  # dropout，防止过拟合
        
        # 节点新的特征向量
        h_prime = torch.matmul(attention, h)  # [N, N].[N, out_features] => [N, out_features]
        
        if self.concat:
            return F.elu(h_prime)
        else:
            return h_prime


AMinerGNN: Heterogeneous Graph Neural Network for paper click-through rate prediction with fusion query

ABSTRACT

用户生成关键字的论文推荐是指推荐既符合用户兴趣又与输入关键字相关的论文。这是一个带有两个查询的推荐任务。用户ID和关键字。然而，现有的方法主要集中在根据一个查询(即用户ID)进行推荐，不适用于解决这一问题。在本文中，我们提出了一种新的基于异构图神经网络的点击率(CTR)预测模型，称为AMinerGNN，用于推荐具有两个查询的论文。

具体来说，AMinerGNN通过图表示学习构建异构图，将用户、论文和关键字投影到同一嵌入空间。为了处理两个查询，设计了一种新的查询关注融合层，动态识别它们的重要性，然后将它们融合为一个查询，构建一个统一的端到端推荐系统。在我们提出的数据集上的实验结果和在线A/B测试证明了AMinerGNN的优越性。

1 INTRODUCTION

![image-20230112205337755](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112205337755.png)

AMiner是一个学术信息检索网站，可以为研究人员提供论文推荐。根据用户是否输入关键字，在线服务有两种推荐系统(RSs)，称为General-RS和keyword - rs(如图1所示)。当同时给出用户ID和用户生成的关键字(通常是研究方向，如图神经网络或预训练)时，keyword - rs推荐的论文不仅与该关键字相关，而且满足用户潜在兴趣。

根据用户生成的关键词与用户基于其交互论文的研究方向之间的关系，在keyword - rs中有三种分割场景:(1)两者都指向同一研究领域(S1);(2)二者之间存在交叉研究的潜力(S2);(3)它们是独立的，没有相关性(S3)。我们收集了一些用户反馈，有一个重要的发现:研究人员使用AMiner的目的和这两个查询的重要性在不同的场景下有所不同，具体如下:(1)在S1中，用户的目的是对该研究领域进行更深入的文献回顾，比如跟踪先进的方法或寻找高被引论文。此时，keyword - rs应该利用自己的兴趣进行推荐，用户ID比关键字更重要，因为用户ID包含个人信息。(2)在S2中，用户想知道如何将这种方法(用户生成的关键字)整合到他过去的研究中，例如如何将对比学习用于推荐。在这种情况下，两个查询都很有用。(3)在S3中，用户更倾向于提高对这个新领域的总体理解(我们称之为新领域，因为这个关键字从未出现在他的交互论文中)，比如自然语言处理领域的Bert[4]。因此，推荐论文时不考虑个人喜好，展示一些被大多数研究人员点击的典型或热门论文是合理的，这意味着关键词比用户ID更重要。为了满足上述推荐要求，有两个问题:(1)如何建立用户生成关键字与其过去研究方向之间的关系模型;(2)如何构建统一的、端到端的、适用于所有场景的推荐系统。

上面的业务是一个典型的点击率(CTR)预测任务，但是有一个新的输入，也就是用户ID和关键字。早期工作通过自动特征交互(如DeepFM [9]， PNN[21]和InterHAt[17])来预测CTR。最近的研究人员旨在建模用户行为(例如DIN [28]， UBR[20]和SIM[19])和时变兴趣(例如DIEN[27]和SURGE[3])。但是，它们都只给出一个查询，也就是用户ID，因此不太适用。另一个接近的解决方案是产品搜索中的方法，将输入关键字作为搜索查询。他们专注于推断个性化搜索倾向(例如HEM[1]和GEPS[26])或分析用户的长期和短期偏好(例如ALSTP[11]和MGDSPR[16])。然而，他们忽略了个性化和搜索意识的重要性是随着场景而动态变化的。

为了解决上述问题，我们提出了一种用于论文点击率(CTR)预测的端到端异构图神经网络，称为AMinerGNN。它由两个部分组成:(1)图嵌入层。为了对keyword - rs中的语义关联进行建模，我们构建了一个由用户、论文和关键字组成的异构图，并通过基于图的表示学习将上述三类实体投影到同一嵌入空间中，这是进一步了解用户生成的关键字与其过去研究方向之间关系的基础。(2)查询关注融合层。我们利用注意单元自动挖掘用户场景特定目的，动态调整两个查询的重要性，并进一步组合为一个查询，构建统一的推荐系统。



3 PRELIMINARIES

在本节中，我们首先阐述了这个问题，然后详细描述了Keyword-RS中使用的异构图的构造。

3.1 Problem Setup

Paper CTR with two queries.

给定集合< U, K, P >，其中U = {u1，···，uM}表示M个用户，K = {k1，···，kT}表示T个关键词，P = {p1，···，PN}表示N篇论文。在这个问题中，用户和关键字都被视为查询，从而得到三种交互数据，分别是用户-论文、关键字-论文和用户与关键字-论文的交互。

以上三种交互作用分别表示为![image-20230112210534234](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112210534234.png)。**yuk−p表示u输入关键字k时是否单击p**, **yu−p表示u在keyword - rs中单击p或在AMiner中搜索p**。**yk−p表示输入关键字为k时是否单击p**。请注意，yu−p同时利用搜索和推荐日志，而其他两个只使用推荐日志。

我们还利用两个广泛使用的信息[15,18,25]来提高CTR性能:(1)查询行为，包括用户行为![image-20230112210953109](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112210953109.png)和关键字行为![image-20230112211010810](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112211010810.png)两个方面，(2)item特征![image-20230112211040457](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112211040457.png)，其中![image-20230112211057621](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112211057621.png)表示论文p的第q个特征，如引用数或出版年份。注意，Hu不考虑关键字，表示你点击过的论文，而Hk表示当输入关键字为k时所有用户都点击过的论文。当用户u输入关键字k时，keyword - rs是预测你是否会点击p作为![image-20230112211134033](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112211134033.png)。

3.2 Graph Construction

我们构造了一个异构图G来保持Keyword-RS中不同实体之间的语义相关性。具体来说，有三种类型的节点:用户、论文和关键字。我们将论文的标题拆分为单个的words，并使用像nltk这样的单词标记化工具过滤出stopword，然后将这些单词收集起来作为关键字集K。

然后根据节点之间的原始交互构建三种边缘类型，如下所示:

• user-paper.当用户u点击或搜索论文p时，添加一条边(u, p)。

• user-keyword: 当用户u添加关键字k时，如图1所示，添加了一条边(u, k)。

• paper-keyword: 当论文p的标题包含关键字k时，添加一条边(p, k)。

![image-20230112213947898](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112213947898.png)

综上所述，G定义为![image-20230112213840535](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112213840535.png)，有节点V和边E。![image-20230112213901776](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112213901776.png)和![image-20230112213911829](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112213911829.png)![image-20230112213924983](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112213924983.png)表示节点类型集合和边类型集合。注意，所有边都表示对称关系，这意味着G是一个无向图。图2(a)显示了上述三种情况的一个示例。

4 METHODOLOGY

在本节中，我们从动机和直觉出发，然后描述所提出的AMinerGNN的技术细节，它遵循一个层次结构:GNN嵌入层→查询注意融合层。图2给出了AMinerGNN的框架

4.1 Motivation

为了解决第1节中提到的两个问题，设计了一个三步解决方案:(1)在同一个嵌入空间中表示用户、论文和关键字→(2)通过实体之间的原始交互增强表示→(3)将用户和关键字表示送入统一的CTR模型。

前两步和最后一步分别在图嵌入层(参见第4.2节)和查询注意融合层(参见第4.3节)中实现。

请注意，GNN具有重要意义。具体来说，在不同的实体方面有三个优势:(1)On the paper side.论文表示法可以包含论文研究方向的信息，将关键词整合到论文表示法中。(2)On the user side.用户交互的论文代表了他过去的研究方向和论文浏览偏好。此外，用户添加的关键词直接反映了他所关注的研究领域。因此，使用上述两个实体增强用户表示非常关键。(3)On the keyword side.如果两个关键词同时出现在一篇论文中，这表明了它们之间跨学科研究的潜力。因此，对关键字的相互增强表示可以使它们在潜在语义空间中更接近。两个关键词嵌入的距离越近，跨学科研究的潜力越大。

4.2 Graph embedding Layer

为了实现前两个步骤，GNN是一种使用多跳邻居信息增强节点表示的直观工具。在每一层中，我们采用了广泛使用的两步方案[8,23,24]来聚合邻居:(1)通过平均池化来聚合相同关系感知的邻居;(2)通过和池化进行关系组合。我们将其他池化方法(如注意力或基于元路径的聚合)的进一步工作留作未来工作。更正式地说，在第l层中，节点表示更新为

![image-20230112214154601](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112214154601.png)

其中e(l) I表示节点I嵌入到第l层，e(0) I是随机初始化的。这里i可以是用户，论文或关键字节点之一。Nti表示节点i和t∈OE的t型邻居。

在执行L层后，我们获得多个节点表示，然后采用和池方式将多跳信息集成为

![image-20230112214344650](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112214344650.png)

其中，上标*表示该表示由GNN增强。

4.3 Query Attentive Fusion Layer

对于同一嵌入空间中的所有表示，我们计算用户和关键字之间的相关性，记为Yuk，使用距离相关性为

![image-20230112214500985](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112214500985.png)

其中d Cov(·)是两个表示的距离协方差。

正如第1节中提到的，两个查询之间的关系对于所有场景都是不一致的。更具体地说，用户ID的重要性与两个查询之间的相关性成比例。因此，一个直观的想法是将 Yuk作为关注权重，形成一个统一融合的查询

![image-20230112214558071](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112214558071.png)

请注意，e * q可以表示不同情况下的用户目的。例如，考虑极限情况，当Yuk = 0，这意味着用户生成的关键字与他过去的研究方向完全无关，e * q = e * k是合理的，因为在这种情况下不需要使用个人兴趣进行推荐。在其他两种情况下也有类似的合理性。

两个查询融合后，另一个问题是如何处理查询行为Hu和Hk。类似于e * u和e * k的融合，启发式思想是将每个行为的部分序列组合为一个新的行为，记为Huk。每个部分行为的长度由Yuk控制。例如，**当Yuk很大的时候，我们希望保留更多的用户行为和更少的关键字行为**，反之亦然。为了更快和统一的张量计算，我们设置Huk的长度为一个固定的数字lℎ。然后⌊Yuk * lℎ⌋最近在Hu的论文和 lℎ−⌊Yuk * lℎ⌋最近在Hk的论文被选为Huk。符号⌊x⌋表示小于或等于x的最大整数。形式上，Huk生成为

![image-20230112215008846](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112215008846.png)

最后，我们将融合查询e * q和行为Huk以及其他辅助信息Fp输入到CTR模块中，预测论文p被点击的概率

![image-20230112215055495](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112215055495.png)

其中fCTR可以是任何CTR模型，默认设置为DIEN[27]。

注意，AMinerGNN可以推广到多查询CTR预测。例如，当用户输入两个关键字时，查询融合的思想仍然是富有成效的，将三个查询(一个用户ID和两个关键字)用心地组合为一个查询，并根据它们的重要性分配三个注意权重，我们将其作为未来的工作。

4.4 Model Optimization

我们使用Logloss，这是CTR预测中最常用的目标函数，以捕获两个概率分布之间的散度

![image-20230112215200467](https://gitee.com/ning13445/picture/raw/master/picture/1/image-20230112215200467.png)

其中s是大小为Z的训练集。